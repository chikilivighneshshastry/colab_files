{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chikilivighneshshastry/colab_files/blob/main/jobdata_api_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "published_since = (datetime.now() - timedelta(days=2)).strftime('%Y-%m-%d')\n",
        "print(published_since)\n",
        "cookies = {\n",
        "    'csrftoken': '5LSZIfHHmDnSIaZKDsN0hGYXaWQhOJGV',\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    'accept': 'application/json',\n",
        "    'accept-language': 'en-US,en;q=0.9',\n",
        "    'priority': 'u=1, i',\n",
        "    'referer': 'https://jobdataapi.com/api/schema/swagger-ui/',\n",
        "    'sec-ch-ua': '\"Google Chrome\";v=\"137\", \"Chromium\";v=\"137\", \"Not/A)Brand\";v=\"24\"',\n",
        "    'sec-ch-ua-mobile': '?0',\n",
        "    'sec-ch-ua-platform': '\"Windows\"',\n",
        "    'sec-fetch-dest': 'empty',\n",
        "    'sec-fetch-mode': 'cors',\n",
        "    'sec-fetch-site': 'same-origin',\n",
        "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36',\n",
        "    # 'cookie': 'csrftoken=5LSZIfHHmDnSIaZKDsN0hGYXaWQhOJGV',\n",
        "}\n",
        "parameters = {\n",
        "    'country_code' : 'IN',\n",
        "    # 'page' : '',\n",
        "    # 'page_size' : 50,\n",
        "    'published_since' : published_since\n",
        "}\n",
        "\n",
        "response = requests.get('https://jobdataapi.com/api/jobs/', cookies=cookies, headers=headers,params=parameters)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "on0LgSdj1GqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# print(json.dumps(response.json()))\n",
        "data = response.json()\n",
        "json.dumps(response.json())"
      ],
      "metadata": {
        "id": "XlWy1u0i1Xo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = data['results']\n",
        "print(results)\n",
        "len(results)"
      ],
      "metadata": {
        "id": "4megXFX41mkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "job_post = results[0]\n",
        "\n",
        "json.dumps(job_post['cities'])"
      ],
      "metadata": {
        "id": "ucIvcnad4BcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "df\n"
      ],
      "metadata": {
        "id": "doOcMIxh22HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# @title experience_level vs salary_min\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "plt.subplots(figsize=(8, 8))\n",
        "df_2dhist = pd.DataFrame({\n",
        "    x_label: grp['salary_min'].value_counts()\n",
        "    for x_label, grp in df.groupby('experience_level')\n",
        "})\n",
        "sns.heatmap(df_2dhist, cmap='viridis')\n",
        "plt.xlabel('experience_level')\n",
        "_ = plt.ylabel('salary_min')"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "Yh9B6oCR8n8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latest_published = df['published'].max()\n",
        "earliest_published = df['published'].min()\n",
        "\n",
        "print(f\"Latest published date: {latest_published}\")\n",
        "print(f\"Earliest published date: {earliest_published}\")\n",
        "\n",
        "# 3. Calculate the difference directly\n",
        "# latest_published and earliest_published are already datetime objects\n",
        "time_difference = latest_published - earliest_published"
      ],
      "metadata": {
        "id": "tp5XNrD18e3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_city_fil = df[df['has_remote'].apply(lambda x: x)]\n",
        "print(df_city_fil)"
      ],
      "metadata": {
        "id": "8zBjqerd6_FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(2)"
      ],
      "metadata": {
        "id": "gX8IQrF7DJxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "company_name = df['company'][0]\n",
        "print(company_name)"
      ],
      "metadata": {
        "id": "n9Wd04zMSrmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b4c6f6d"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the sqlite3 library, define the database file name, establish a connection, and create a cursor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7801a5eb"
      },
      "source": [
        "import sqlite3\n",
        "\n",
        "db = './jobs_data.db'\n",
        "conn = sqlite3.connect(db)\n",
        "cursor = conn.cursor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39443c4b"
      },
      "source": [
        "## Create table if not exists\n",
        "\n",
        "### Subtask:\n",
        "Define and create the table in the database with appropriate columns if it doesn't already exist.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d83f261"
      },
      "source": [
        "**Reasoning**:\n",
        "Define and create the table in the database with appropriate columns if it doesn't already exist.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51caea9a"
      },
      "source": [
        "create_table_sql = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS raw_jobdataapi_data (\n",
        "    id INTEGER PRIMARY KEY,\n",
        "    ext_id TEXT,\n",
        "    company TEXT,\n",
        "    title TEXT,\n",
        "    location TEXT,\n",
        "    types TEXT,\n",
        "    cities TEXT,\n",
        "    states TEXT,\n",
        "    countries TEXT,\n",
        "    regions TEXT,\n",
        "    has_remote BOOLEAN,\n",
        "    published datetime,\n",
        "    description TEXT,\n",
        "    experience_level TEXT,\n",
        "    application_url TEXT,\n",
        "    language TEXT,\n",
        "    salary_min INTEGER,\n",
        "    salary_max INTEGER,\n",
        "    salary_currency TEXT,\n",
        "\n",
        "    verified BOOLEAN default False,\n",
        "    page_content_html TEXT,\n",
        "    is_expried BOOLEAN,\n",
        "    created_at datetime default CURRENT_TIMESTAMP,\n",
        "    updated_at datetime default CURRENT_TIMESTAMP\n",
        ")\n",
        "\"\"\"\n",
        "cursor.execute(create_table_sql)\n",
        "conn.commit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2d2583a"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the DataFrame, check for existing records by id, and insert new records into the SQLite database, converting complex types to JSON strings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1403f1a"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "published_since = (datetime.now() - timedelta(days=2)).strftime('%Y-%m-%d')\n",
        "# published_since = datetime.now() - timedelta(days=2)\n",
        "db = './jobs_data.db'\n",
        "conn = sqlite3.connect(db)\n",
        "\n",
        "# df = df[df['published']<20min] # correct this code\n",
        "\n",
        "# Get existing IDs from the database\n",
        "existing_ids = pd.read_sql(f\"SELECT id FROM raw_jobdataapi_data where published > {published_since}\", conn)['id'].tolist() # get last 20 min id's\n",
        "\n",
        "# Filter out rows that already exist in the database\n",
        "df_to_insert = df[~df['id'].isin(existing_ids)].copy()\n",
        "\n",
        "# Convert complex data types to JSON strings in the DataFrame\n",
        "for col in ['company', 'types', 'cities', 'states', 'countries', 'regions']:\n",
        "    df_to_insert[col] = df_to_insert[col].apply(lambda x: json.dumps(x) if isinstance(x, (dict, list)) else None)\n",
        "\n",
        "# Insert the filtered DataFrame into the SQLite database\n",
        "df_to_insert.to_sql('raw_jobdataapi_data', conn, if_exists='append', index=False)\n",
        "\n",
        "conn.commit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08909864"
      },
      "source": [
        "**Reasoning**:\n",
        "Close the connection to the SQLite database.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99f381ee"
      },
      "source": [
        "conn.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright\n",
        "!playwright install"
      ],
      "metadata": {
        "id": "O9oyA0rhnWZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# playright code"
      ],
      "metadata": {
        "id": "r9B8TGYMLqqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: get all jobs with verified = false read from db and verifiy job posts going to url using playright and update db with page_content_html and is_expried status based upon title in content (expried or not) and change verifiy to true\n",
        "\n",
        "# !pip install playwright\n",
        "# !playwright install\n",
        "\n",
        "import sqlite3\n",
        "from playwright.sync_api import sync_playwright\n",
        "import re\n",
        "\n",
        "db = './jobs_data.db'\n",
        "conn = sqlite3.connect(db)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get all jobs with verified = false\n",
        "cursor.execute(\"SELECT id, application_url FROM raw_jobdataapi_data WHERE verified = FALSE\")\n",
        "jobs_to_verify = cursor.fetchall()\n",
        "\n",
        "p = sync_playwright().start()\n",
        "browser = p.chromium.launch(headless=True)\n",
        "page = browser.new_page()\n",
        "\n",
        "for job_id, job_url in jobs_to_verify:\n",
        "    try:\n",
        "        page.goto(job_url, timeout=60000)  # Set a timeout for page loading\n",
        "        page_content = page.content()\n",
        "\n",
        "        # Check for \"expired\" in the title or content\n",
        "        is_expired = bool(re.search(r'(expired|closed|no longer available)', page_content, re.IGNORECASE))\n",
        "\n",
        "        # Update the database\n",
        "        update_sql = \"\"\"\n",
        "        UPDATE raw_jobdataapi_data\n",
        "        SET verified = TRUE,\n",
        "            page_content_html = ?,\n",
        "            is_expried = ?,\n",
        "            updated_at = CURRENT_TIMESTAMP\n",
        "        WHERE id = ?\n",
        "        \"\"\"\n",
        "        cursor.execute(update_sql, (page_content, is_expired, job_id))\n",
        "        conn.commit()\n",
        "        print(f\"Verified job {job_id} at {job_url}. Expired status: {is_expired}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error verifying job {job_id} at {job_url}: {e}\")\n",
        "        # Optionally, you might want to log this error or mark the job differently in the DB\n",
        "\n",
        "browser.close()\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "XQCzvno4kblc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import asyncio # Import asyncio\n",
        "from playwright.async_api import async_playwright # Import async_playwright\n",
        "import re\n",
        "\n",
        "# Database configuration\n",
        "db = './jobs_data.db'\n",
        "\n",
        "# Define the async function to run the Playwright operations\n",
        "async def verify_job_posts():\n",
        "    conn = sqlite3.connect(db)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Get all jobs with verified = FALSE\n",
        "    # Ensure your table has a 'verified' column (e.g., INTEGER, 0 for FALSE, 1 for TRUE)\n",
        "    # And 'page_content_html' (TEXT), 'is_expired' (INTEGER), 'updated_at' (DATETIME)\n",
        "    cursor.execute(\"SELECT id, application_url FROM raw_jobdataapi_data WHERE verified = 0\")\n",
        "    jobs_to_verify = cursor.fetchall()\n",
        "\n",
        "    if not jobs_to_verify:\n",
        "        print(\"No unverified jobs found to process.\")\n",
        "        conn.close()\n",
        "        return\n",
        "\n",
        "    # Use async_playwright for async operations\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=False)\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        for job_id, job_url in jobs_to_verify:\n",
        "            try:\n",
        "                print(f\"Attempting to verify job {job_id} at {job_url}...\")\n",
        "                await page.goto(job_url, timeout=60000)  # Set a timeout for page loading\n",
        "                page_content = await page.content()\n",
        "\n",
        "                # Check for \"expired\" in the title or content\n",
        "                # Added a check for common \"not found\" or \"404\" page indicators\n",
        "                is_expired = bool(re.search(r'(expired|closed|no longer available|404 Not Found|Page Not Found)', page_content, re.IGNORECASE))\n",
        "                # Also check page status for explicit 404/500 errors\n",
        "                if page.status != 200:\n",
        "                    is_expired = True\n",
        "\n",
        "                # Update the database\n",
        "                update_sql = \"\"\"\n",
        "                UPDATE raw_jobdataapi_data\n",
        "                SET verified = ?, -- Use 1 for TRUE\n",
        "                    page_content_html = ?,\n",
        "                    is_expried = ?, -- Use 1 for TRUE, 0 for FALSE\n",
        "                    updated_at = CURRENT_TIMESTAMP\n",
        "                WHERE id = ?\n",
        "                \"\"\"\n",
        "                # SQLite stores booleans as 0 (False) or 1 (True)\n",
        "                cursor.execute(update_sql, (1, page_content, 1 if is_expired else 0, job_id))\n",
        "                conn.commit()\n",
        "                print(f\"Verified job {job_id}. Expired status: {is_expired}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error verifying job {job_id} at {job_url}: {e}\")\n",
        "                # You might want to update the 'verified' status to indicate a failure\n",
        "                # e.g., verified = -1 (for error) or add an 'error_message' column\n",
        "                # For now, we just print the error and continue\n",
        "                conn.rollback() # Rollback any partial updates for this job if an error occurred\n",
        "\n",
        "        await browser.close()\n",
        "        print(\"Browser closed.\")\n",
        "\n",
        "    conn.close()\n",
        "    print(\"Database connection closed.\")\n",
        "\n",
        "# Run the async function\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a dummy table for demonstration if it doesn't exist\n",
        "\n",
        "    # This is how you run an async function in a synchronous context\n",
        "    await verify_job_posts()"
      ],
      "metadata": {
        "id": "3aMtC5Dqx1tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Update predefined tabels\n",
        "\n",
        "__________\n",
        "\n",
        "## All locations in a table\n",
        "* save countries , states and citys in db along with codes like country code and similar\n",
        "\n",
        "## All skills in skills table\n",
        "- get skills details from `https://www.kaggle.com/datasets/arbazkhan971/allskillandnonskill/data?select=skills.csv` and update db\n",
        "\n",
        "# Company\n",
        "- get compnay details from Jobdata_api or any other source and update db\n",
        "\n",
        "# Job category\n",
        "- update db with all predefined job category\n",
        "\n",
        "> ## how to handle if any of the above data we get from api is missmatching or not avalibale with us  \n",
        ">> mostly we will have issue with `# Location and company details`  handle that.  \n",
        "> like by add new details in db or identiflying similar name with any method\n",
        "\n",
        "> need to use techniques like.  \n",
        "1 Fuzzy matching.  \n",
        "2 third-party services to match or online free data set.   \n",
        "3 ML model\n",
        "\n",
        "# Other tables :\n",
        "  ### Job Type\n",
        "\n",
        "- Full-time\n",
        "\n",
        "- Contract\n",
        "\n",
        "- Part-time\n",
        "\n",
        "- Internship\n",
        "\n",
        "### Work Model\n",
        "\n",
        "- Onsite\n",
        "\n",
        "- Remote\n",
        "\n",
        "- Hybrid\n",
        "\n",
        "### Experience Level\n",
        "\n",
        "- Intern/New Grad\n",
        "notes\n",
        "\n",
        "- Entry Level\n",
        "notes\n",
        "\n",
        "- Mid Level\n",
        "notes\n",
        "\n",
        "- Senior Level\n",
        "notes\n",
        "\n",
        "- Lead/Staff\n",
        "notes\n",
        "\n",
        "- Director/Executive\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HP3BbkeQ7r_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first go to url with request lib or any async lib and update db and make 2nd flow with playwright as below\n",
        "#get all jobs with verified = false and verifiy job posts going to url using playwright and update db with page_content_html and is_expried status based upon title in content (expried or not) and change verifiy to true\n",
        "\n",
        "# read data from this raw_table (last 2 days data)\n",
        "# read main_jobs_data table (last 2 days data)\n",
        "# compare both by id and url and remove commen\n",
        "# for unique run data_manupulation worker --> based upon city name get\n",
        "# get city and (location details) , work_type,clean published (date) and check if date is behyod the current data, from json\n",
        "# description -->html to text , company details from json to compnay table mapping\n",
        "# send data to ai to tag a category,get skills,yeas_of_exp (min and max),musthave qulification,prefred qulifications,,\n",
        "\n",
        "#save this data in main jobs table and also inject data in main jobs dependcy tables like category and jobpost matching"
      ],
      "metadata": {
        "id": "GEeNydDCQNL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# when user request data\n",
        "# get all jobpost from main jobs table after filtering category (selected by user)\n",
        "# check if this jobposts already in user_score table if not run scoring worker and append data to table\n",
        "# send data to front end\n",
        "# delete expried jobpost fromt his table when delete_expried_worker run"
      ],
      "metadata": {
        "id": "JeTLR0oNYKsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "db = './jobs_data.db'\n",
        "conn = sqlite3.connect(db)\n",
        "cursor = conn.cursor()\n",
        "create_table_sql = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS job_posts (\n",
        "    id TEXT PRIMARY KEY,\n",
        "    jobtitle TEXT,\n",
        "    jobdataapi_id TEXT,\n",
        "    title TEXT,\n",
        "    location TEXT,\n",
        "    jobdescription TEXT,\n",
        "    company_id TEXT,\n",
        "    published_date TEXT, -- Storing ISO 8601 strings, e.g., 'YYYY-MM-DD HH:MM:SS'\n",
        "    published_time_ux INTEGER, -- Unix timestamp (seconds since epoch)\n",
        "    experience_level TEXT,\n",
        "    salary_min INTEGER,\n",
        "    salary_max INTEGER,\n",
        "    salary_currency TEXT,\n",
        "    job_language TEXT,\n",
        "    application_url TEXT,\n",
        "    country_id TEXT,\n",
        "    -- Define Foreign Key constraint (optional if company_details table isn't created yet)\n",
        "    FOREIGN KEY (company_id) REFERENCES company_details(id)\n",
        "        ON DELETE NO ACTION ON UPDATE NO ACTION\n",
        ");\n",
        "\"\"\"\n",
        "# Execute the SQL command\n",
        "cursor.execute(create_table_sql)\n",
        "conn.commit()\n",
        "print(f\"Table 'job_posts' created successfully in {db}\")"
      ],
      "metadata": {
        "id": "mgG49rVeY2EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_table_ids = pd.read_sql(f\"SELECT jobdataapi_id,application_url FROM job_posts where published > {published_since}\", conn)['jobdataapi_id'].tolist()\n",
        "raw_table_ids = pd.read_sql(f\"SELECT id FROM raw_jobdataapi_data where published > {published_since}\", conn)['id'].tolist()\n",
        "# Filter out rows that already exist in the database\n",
        "df_to_insert = df[~df['id'].isin(existing_ids)].copy()"
      ],
      "metadata": {
        "id": "28elV8TLcZTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classify Job category using job title and JD similarity score with our category data in table\n",
        "\n",
        "\n",
        "      This is very Importent if need accuracy then use better embading model or fine-tune embadding model for accuracy\n",
        "\n",
        "-------------------------------------"
      ],
      "metadata": {
        "id": "ZwVHcGGzJb1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Need to extact from JD\n",
        "------------\n",
        "\n",
        "- Responsibilities  \n",
        "- Qualification -\n",
        "              1 Required\n",
        "             2 preferred\n",
        "- Benifits\n",
        "----------\n",
        "- Years of Experiance -- `Min_y and Max_y`\n",
        "- skills -- > can be extracted using our skills table\n",
        "- if remort in page mark it as remote\n",
        "- Salary"
      ],
      "metadata": {
        "id": "omGMSjKzGXg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install httpx aiohttp requests"
      ],
      "metadata": {
        "id": "uIkd31Obol9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# --- Function to Clean HTML into Plain Text ---\n",
        "def clean_html_text(html_content: str) -> str:\n",
        "    \"\"\"\n",
        "    Cleans HTML content by removing tags and normalizing whitespace.\n",
        "    \"\"\"\n",
        "    if not html_content:\n",
        "        return \"\"\n",
        "    try:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Get all the text\n",
        "        text = soup.get_text()\n",
        "\n",
        "        # Break into lines and remove leading/trailing space on each\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "\n",
        "        # Break multi-word lines into phrases and join them with a single space\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "\n",
        "        # Join everything back together with a single space and remove any final excess whitespace\n",
        "        cleaned_text = ' '.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "        return cleaned_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error cleaning text: {e}\")\n",
        "        return \"Could not clean HTML content.\"\n",
        "\n",
        "\n",
        "# --- Asynchronous Function to Fetch a URL ---\n",
        "async def fetch_url(session: aiohttp.ClientSession, url: str):\n",
        "    \"\"\"\n",
        "    Asynchronously fetches a single URL and returns the URL and its text content.\n",
        "    Includes error handling for network or HTTP issues.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # The 'async with' statement ensures the connection is closed properly.\n",
        "        async with session.get(url, timeout=10) as response:\n",
        "            # raise_for_status() will raise an exception for 4xx/5xx responses\n",
        "            response.raise_for_status()\n",
        "            print(f\"SUCCESS: Fetched {url} with status {response.status}\")\n",
        "            html_content = await response.text()\n",
        "            return url, html_content\n",
        "    except aiohttp.ClientError as e:\n",
        "        # Handles client-side errors (e.g., connection error, timeout)\n",
        "        print(f\"ERROR: Could not fetch {url}. Reason: {e}\")\n",
        "        return url, None\n",
        "    except Exception as e:\n",
        "        # Handles other potential errors\n",
        "        print(f\"UNEXPECTED ERROR for {url}. Reason: {e}\")\n",
        "        return url, None\n",
        "\n",
        "\n",
        "# --- Main Orchestrator Function ---\n",
        "async def main():\n",
        "    \"\"\"\n",
        "    Main function to run the fetching and processing concurrently.\n",
        "    \"\"\"\n",
        "    # List of 10 URLs to fetch.\n",
        "    # I've included a delayed URL to demonstrate the \"as they arrive\" behavior.\n",
        "    urls = [\n",
        "        \"http://python.org\",\n",
        "        \"https://www.google.com\",\n",
        "        \"https://github.com\",\n",
        "        \"https://www.wikipedia.org\",\n",
        "        \"https://httpbin.org/delay/3\",  # This one will intentionally finish later\n",
        "        \"https://aiohttp.readthedocs.io\",\n",
        "        \"https://www.python-requests.org\",\n",
        "        \"https://stackoverflow.com\",\n",
        "        \"https://httpbin.org/status/404\", # This one will fail\n",
        "        \"https://developer.mozilla.org\",\n",
        "    ]\n",
        "\n",
        "    # Create a DataFrame with url, url_id, and url_active\n",
        "    df = pd.DataFrame(urls, columns=['url'])\n",
        "    df['url_id'] = df.index\n",
        "    df['url_active'] = False\n",
        "    df['page_content'] = ''\n",
        "\n",
        "    # aiohttp recommends a single session for all requests\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        # Create a list of tasks (coroutines) to run\n",
        "        tasks = [fetch_url(session, url) for url in df['url']]\n",
        "\n",
        "        # --- The Magic: asyncio.as_completed() ---\n",
        "        # This function returns an iterator that yields futures as they complete.\n",
        "        # This is exactly what you want: process results as they arrive.\n",
        "        print(\"--- Starting to fetch URLs. Output will appear as it arrives ---\")\n",
        "\n",
        "        for future in asyncio.as_completed(tasks):\n",
        "            # We 'await' the future to get its result (url, html_content)\n",
        "            url, html_content = await future\n",
        "\n",
        "            if html_content:\n",
        "                # If we got content, clean it and update the DataFrame\n",
        "                cleaned_text = clean_html_text(html_content)\n",
        "\n",
        "                # Find the row for the URL and update it\n",
        "                df.loc[df['url'] == url, ['url_active', 'page_content']] = [True, cleaned_text]\n",
        "\n",
        "                print(\"-\" * 50)\n",
        "                print(f\"✅ RESULT for: {url}\")\n",
        "                print(f\"Cleaned Content (first 200 chars): {cleaned_text[:200]}...\")\n",
        "                print(\"-\" * 50, \"\\n\")\n",
        "            else:\n",
        "                # If fetching failed, html_content will be None\n",
        "                # url_active remains False\n",
        "                print(\"-\" * 50)\n",
        "                print(f\"❌ FAILED for: {url}\")\n",
        "                print(\"-\" * 50, \"\\n\")\n",
        "\n",
        "    print(\"\\n--- Final DataFrame ---\")\n",
        "    print(df)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This is the standard way to run an asyncio program\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "nW11X0Uwohom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import httpx\n",
        "import aiohttp\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "class AsyncThreadPoolBenchmark:\n",
        "    def __init__(self, urls: List[str], num_requests: int = 100):\n",
        "        self.urls = urls\n",
        "        self.num_requests = num_requests\n",
        "\n",
        "    # AIOHTTP - Pure Async (no threads)\n",
        "    async def aiohttp_async_test(self) -> Dict[str, Any]:\n",
        "        errors = 0\n",
        "\n",
        "        connector = aiohttp.TCPConnector(limit=100, limit_per_host=50)\n",
        "        timeout = aiohttp.ClientTimeout(total=30)\n",
        "\n",
        "        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n",
        "            start_time = time.time()\n",
        "\n",
        "            async def fetch_url(session, url):\n",
        "                try:\n",
        "                    async with session.get(url) as response:\n",
        "                        content = await response.read()\n",
        "                        return response.status, len(content)\n",
        "                except Exception as e:\n",
        "                    return None, str(e)\n",
        "\n",
        "            tasks = []\n",
        "            for i in range(self.num_requests):\n",
        "                url = self.urls[i % len(self.urls)]\n",
        "                tasks.append(fetch_url(session, url))\n",
        "\n",
        "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "            end_time = time.time()\n",
        "\n",
        "            for result in results:\n",
        "                if isinstance(result, Exception) or (hasattr(result, '__len__') and len(result) == 2 and result[0] is None):\n",
        "                    errors += 1\n",
        "\n",
        "            total_time = end_time - start_time\n",
        "\n",
        "        return {\n",
        "            'total_time': total_time,\n",
        "            'requests_per_second': self.num_requests / total_time,\n",
        "            'avg_time_per_request': total_time / self.num_requests,\n",
        "            'errors': errors,\n",
        "            'successful_requests': self.num_requests - errors\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    async def run_benchmark(self, thread_workers: List[int] = [4]) -> Dict[str, Dict[str, Any]]:\n",
        "        print(f\"Running benchmark with {self.num_requests} requests to {len(self.urls)} URLs\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # Test AIOHTTP Pure Async\n",
        "        print(\"Testing AIOHTTP (Pure Async - no threads)...\")\n",
        "        results['aiohttp_async'] = await self.aiohttp_async_test()\n",
        "\n",
        "\n",
        "        # # Test with different thread pool sizes\n",
        "        # for workers in thread_workers:\n",
        "        #     print(f\"Testing AIOHTTP with ThreadPool ({workers} workers)...\")\n",
        "        #     results[f'aiohttp_threads_{workers}'] = await self.aiohttp_threadpool_test(workers)\n",
        "\n",
        "        #     print(f\"Testing HTTPX with ThreadPool ({workers} workers)...\")\n",
        "        #     results[f'httpx_threads_{workers}'] = await self.httpx_threadpool_test(workers)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def print_results(self, results: Dict[str, Dict[str, Any]]):\n",
        "        print(\"\\n\" + \"=\" * 90)\n",
        "        print(\"ASYNC vs THREADPOOL BENCHMARK RESULTS\")\n",
        "        print(\"=\" * 90)\n",
        "\n",
        "        # Sort by requests per second (descending)\n",
        "        sorted_results = sorted(results.items(),\n",
        "                              key=lambda x: x[1]['requests_per_second'],\n",
        "                              reverse=True)\n",
        "\n",
        "        print(f\"{'Method':<25} {'Workers':<8} {'Total Time':<12} {'Req/Sec':<12} {'Avg/Req':<12} {'Errors':<8}\")\n",
        "        print(\"-\" * 90)\n",
        "\n",
        "        for method, data in sorted_results:\n",
        "            workers = data.get('max_workers', 'N/A')\n",
        "            print(f\"{method:<25} \"\n",
        "                  f\"{workers:<8} \"\n",
        "                  f\"{data['total_time']:<12.2f} \"\n",
        "                  f\"{data['requests_per_second']:<12.2f} \"\n",
        "                  f\"{data['avg_time_per_request']:<12.4f} \"\n",
        "                  f\"{data['errors']:<8}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 90)\n",
        "        winner = sorted_results[0]\n",
        "        print(f\"WINNER: {winner[0].upper()}\")\n",
        "        print(f\"Best performance: {winner[1]['requests_per_second']:.2f} requests/second\")\n",
        "\n",
        "        # Analysis\n",
        "        print(\"\\n\" + \"=\" * 90)\n",
        "        print(\"ANALYSIS:\")\n",
        "\n",
        "        # Compare async vs threaded for each library\n",
        "        aiohttp_async = results.get('aiohttp_async', {}).get('requests_per_second', 0)\n",
        "        # httpx_async = results.get('httpx_async', {}).get('requests_per_second', 0)\n",
        "\n",
        "        # # Find best threaded performance for each\n",
        "        # aiohttp_threaded_best = max([v['requests_per_second'] for k, v in results.items() if k.startswith('aiohttp_threads_')], default=0)\n",
        "        # httpx_threaded_best = max([v['requests_per_second'] for k, v in results.items() if k.startswith('httpx_threads_')], default=0)\n",
        "\n",
        "        # print(f\"AIOHTTP: Pure Async ({aiohttp_async:.2f} req/s) vs Best Threaded ({aiohttp_threaded_best:.2f} req/s)\")\n",
        "        # if aiohttp_threaded_best > aiohttp_async:\n",
        "        #     improvement = (aiohttp_threaded_best / aiohttp_async - 1) * 100\n",
        "        #     print(f\"  → Threading is {improvement:.1f}% FASTER for AIOHTTP\")\n",
        "        # else:\n",
        "        #     improvement = (aiohttp_async / aiohttp_threaded_best - 1) * 100\n",
        "        #     print(f\"  → Pure Async is {improvement:.1f}% FASTER for AIOHTTP\")\n",
        "\n",
        "        # print(f\"HTTPX: Pure Async ({httpx_async:.2f} req/s) vs Best Threaded ({httpx_threaded_best:.2f} req/s)\")\n",
        "        # if httpx_threaded_best > httpx_async:\n",
        "        #     improvement = (httpx_threaded_best / httpx_async - 1) * 100\n",
        "        #     print(f\"  → Threading is {improvement:.1f}% FASTER for HTTPX\")\n",
        "        # else:\n",
        "        #     improvement = (httpx_async / httpx_threaded_best - 1) * 100\n",
        "        #     print(f\"  → Pure Async is {improvement:.1f}% FASTER for HTTPX\")\n",
        "\n",
        "        print(\"\\nKEY INSIGHTS:\")\n",
        "        print(\"- Pure async is more memory efficient (no thread overhead)\")\n",
        "        print(\"- ThreadPool can be faster when async libraries have connection limits\")\n",
        "        print(\"- Threading adds overhead but can bypass some async bottlenecks\")\n",
        "        print(\"- Optimal thread count depends on network latency and server response times\")\n",
        "        print(\"=\" * 90)\n",
        "\n",
        "async def main():\n",
        "    # Test URLs - mix of different response times\n",
        "    test_urls = [\n",
        "        \"https://httpbin.org/json\",\n",
        "        \"https://jsonplaceholder.typicode.com/posts/1\",\n",
        "        \"https://api.github.com/users/octocat\",\n",
        "        \"https://httpbin.org/delay/0.1\",\n",
        "        \"https://jsonplaceholder.typicode.com/users/1\",\n",
        "        \"https://httpbin.org/uuid\",\n",
        "    ]\n",
        "\n",
        "    # Create benchmark instance\n",
        "    benchmark = AsyncThreadPoolBenchmark(test_urls, num_requests=1000)\n",
        "\n",
        "    # Run the benchmark with different thread counts\n",
        "    results = await benchmark.run_benchmark(thread_workers=[1])\n",
        "\n",
        "    # Print results\n",
        "    benchmark.print_results(results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Install required packages:\n",
        "    # pip install httpx aiohttp requests\n",
        "\n",
        "    print(\"Starting Async vs ThreadPool Benchmark...\")\n",
        "    print(\"This will test pure async vs threading approaches for HTTP requests\")\n",
        "    print()\n",
        "\n",
        "    await main()"
      ],
      "metadata": {
        "id": "9iZ5gbAAsw22"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}