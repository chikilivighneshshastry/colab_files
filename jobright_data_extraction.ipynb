{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chikilivighneshshastry/colab_files/blob/main/jobright_data_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# extract fingle page info from jonright\n"
      ],
      "metadata": {
        "id": "U5Ooor4StP2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import aiohttp\n",
        "import asyncio\n",
        "\n",
        "async def extract_job_data(url):\n",
        "  async with aiohttp.ClientSession() as session:\n",
        "    response = await session.get(url)\n",
        "    html = await response.text()\n",
        "    print(html)\n",
        "    return response\n",
        "\n",
        "url = 'https://jobright.ai/jobs/info/685a54b5be2d7e56476268d'\n",
        "# url = 'https://jobright.ai/jobs/info'\n",
        "response = await extract_job_data(url)"
      ],
      "metadata": {
        "id": "e1QKrxJOkCEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.status\n",
        "html_data = await response.text()\n",
        "print(html_data)"
      ],
      "metadata": {
        "id": "3k8MLjR-kFBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: parse html_data using bs4 and get with text in a id\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "soup = BeautifulSoup(html_data, 'html.parser')\n",
        "\n",
        "# Assuming the text you want is within an element with a specific ID,\n",
        "# replace 'your_element_id' with the actual ID of the element.\n",
        "element_with_id = soup.find(id='__NEXT_DATA__')\n",
        "\n",
        "if element_with_id:\n",
        "  detailed_json_data = element_with_id.get_text()\n",
        "  print(detailed_json_data)\n",
        "else:\n",
        "  print(\"Element with the specified ID not found.\")\n",
        "\n",
        "data = json.loads(detailed_json_data)\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "URqMe0gEkIGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.keys()\n",
        "print(data['props'].keys())\n",
        "print(data['page'])\n",
        "print(data['query'])\n",
        "print(data['buildId'])\n",
        "print(data['isFallback'])\n",
        "print(data['gssp'])\n",
        "print(data['scriptLoader'])"
      ],
      "metadata": {
        "id": "ol3dES5el8uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['props']['pageProps']['baseSalary'])\n",
        "print(data['props']['pageProps']['jobLocation'])\n",
        "print(data['props']['pageProps']['logined'])\n",
        "print(data['props']['pageProps']['jobHashedId'])\n",
        "print(data['props']['pageProps']['_sentryTraceData'])\n",
        "print(data['props']['pageProps']['_sentryBaggage'])"
      ],
      "metadata": {
        "id": "7G4nb_jAm9P1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['props']['pageProps']['dataSource']"
      ],
      "metadata": {
        "id": "eKJpjkecmnss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import random\n",
        "\n",
        "# --- Configuration ---\n",
        "TARGET_SITE_DOMAIN = \"example.com\" # To keep crawling within the site\n",
        "INITIAL_SEED_URL = f\"http://{TARGET_SITE_DOMAIN}\"\n",
        "\n",
        "PROXY_LIST = [\n",
        "    \"http://198.46.172.102:12345\",\n",
        "    \"http://103.85.103.1:5678\",\n",
        "    # ... more proxies\n",
        "]\n",
        "\n",
        "# --- Database (Conceptual - replace with actual DB interaction) ---\n",
        "# In a real scenario, use libraries like psycopg2 (PostgreSQL), mysql.connector, sqlite3, or an ORM like SQLAlchemy\n",
        "DATABASE_URLS_SEEN = set() # Simple in-memory set for this example; use a real DB!\n",
        "\n",
        "async def db_url_exists(url):\n",
        "    # Simulate DB check\n",
        "    return url in DATABASE_URLS_SEEN\n",
        "\n",
        "async def db_add_url(url):\n",
        "    # Simulate DB add\n",
        "    DATABASE_URLS_SEEN.add(url)\n",
        "    print(f\"[DB] Added: {url}\")\n",
        "\n",
        "# --- Crawler Components ---\n",
        "url_frontier = asyncio.Queue()\n",
        "processed_urls_count = 0\n",
        "MAX_URLS_TO_CRAWL = 100 # Example limit\n",
        "\n",
        "async def fetch(session, url, proxy):\n",
        "    try:\n",
        "        print(f\"[FETCHING] {url} via proxy {proxy if proxy else 'DIRECT'}\")\n",
        "        async with session.get(url, proxy=proxy, timeout=10, ssl=False) as response: # Added ssl=False for potential local SSL issues\n",
        "            if response.status == 200:\n",
        "                return await response.text()\n",
        "            else:\n",
        "                print(f\"[ERROR] HTTP {response.status} for {url}\")\n",
        "                return None\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to fetch {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_proxy():\n",
        "    if PROXY_LIST:\n",
        "        return random.choice(PROXY_LIST)\n",
        "    return None\n",
        "\n",
        "def parse_and_extract_links(html_content, base_url):\n",
        "    links = set()\n",
        "    if not html_content:\n",
        "        return links\n",
        "    soup = BeautifulSoup(html_content, 'lxml') # 'html.parser' is a built-in alternative\n",
        "    for a_tag in soup.find_all('a', href=True):\n",
        "        href = a_tag['href']\n",
        "        # Join relative URLs with the base URL\n",
        "        full_url = urljoin(base_url, href)\n",
        "        # Basic clean-up (remove fragment, normalize)\n",
        "        parsed_url = urlparse(full_url)\n",
        "        normalized_url = parsed_url._replace(fragment=\"\").geturl()\n",
        "\n",
        "        # Filter: Only crawl URLs from the target domain\n",
        "        if urlparse(normalized_url).netloc == TARGET_SITE_DOMAIN:\n",
        "            links.add(normalized_url)\n",
        "    return links\n",
        "\n",
        "async def worker(name, session):\n",
        "    global processed_urls_count\n",
        "    while True:\n",
        "        try:\n",
        "            current_url = await url_frontier.get()\n",
        "            print(f\"[{name}] Processing: {current_url}\")\n",
        "\n",
        "            if await db_url_exists(current_url):\n",
        "                print(f\"[{name}] Already processed/in DB: {current_url}\")\n",
        "                url_frontier.task_done()\n",
        "                continue\n",
        "\n",
        "            await db_add_url(current_url) # Add to DB before fetching (or mark as being processed)\n",
        "\n",
        "            # TODO: Implement robots.txt check here\n",
        "\n",
        "            proxy = get_proxy()\n",
        "            html = await fetch(session, current_url, proxy)\n",
        "\n",
        "            if html:\n",
        "                new_links = parse_and_extract_links(html, current_url)\n",
        "                for link in new_links:\n",
        "                    if not await db_url_exists(link) and processed_urls_count < MAX_URLS_TO_CRAWL :\n",
        "                        # Check DB again before adding to frontier to handle race conditions if multiple workers find same link\n",
        "                        if link not in DATABASE_URLS_SEEN: # Simplified check; real DB would handle uniqueness\n",
        "                            await url_frontier.put(link)\n",
        "                            print(f\"[{name}] Queued new link: {link}\")\n",
        "\n",
        "\n",
        "                processed_urls_count += 1\n",
        "                if processed_urls_count >= MAX_URLS_TO_CRAWL:\n",
        "                    print(f\"[{name}] Reached max URL limit. Draining queue...\")\n",
        "                    # Allow other tasks to finish current work, then stop adding new ones.\n",
        "                    # Or more abruptly, cancel other tasks.\n",
        "\n",
        "            url_frontier.task_done()\n",
        "\n",
        "            if processed_urls_count >= MAX_URLS_TO_CRAWL and url_frontier.empty():\n",
        "                break # Exit worker if limit reached and queue is empty\n",
        "\n",
        "            await asyncio.sleep(1) # Be respectful: add a small delay\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[{name}] Error in worker: {e}\")\n",
        "            url_frontier.task_done() # Ensure task_done is called even on error\n",
        "            continue # Continue to next URL\n",
        "\n",
        "async def main():\n",
        "    await url_frontier.put(INITIAL_SEED_URL)\n",
        "    await db_add_url(INITIAL_SEED_URL) # Add seed to DB initially\n",
        "\n",
        "    # You might want a ClientSession per proxy type or a more sophisticated setup\n",
        "    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=False)) as session: # ssl=False for local dev; use proper SSL context in prod\n",
        "        # Create worker tasks\n",
        "        num_workers = 5 # Number of concurrent crawlers\n",
        "        tasks = []\n",
        "        for i in range(num_workers):\n",
        "            task = asyncio.create_task(worker(f\"Worker-{i+1}\", session))\n",
        "            tasks.append(task)\n",
        "\n",
        "        # Wait for the queue to be processed or limit to be reached\n",
        "        await url_frontier.join() # Waits until all items in queue are gotten and processed\n",
        "\n",
        "        # If max URLs reached, there might still be items in the queue\n",
        "        # or workers might be processing. We need a way to signal them to stop gracefully.\n",
        "        # For simplicity here, we cancel tasks if max_urls is hit and queue is effectively drained by workers.\n",
        "        if processed_urls_count >= MAX_URLS_TO_CRAWL:\n",
        "            print(\"Max URL limit reached. Cancelling worker tasks...\")\n",
        "\n",
        "        for task in tasks:\n",
        "            task.cancel() # Cancel all worker tasks\n",
        "\n",
        "        await asyncio.gather(*tasks, return_exceptions=True) # Wait for tasks to be cancelled\n",
        "\n",
        "    print(\"Crawling finished.\")\n",
        "    print(f\"Total unique URLs seen (from in-memory set): {len(DATABASE_URLS_SEEN)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "HoLY9OQf0Dlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crawl4ai.deep_crawling import BFSDeepCrawlStrategy\n",
        "\n",
        "strategy = BFSDeepCrawlStrategy(\n",
        "    max_depth=2,               # Crawl initial page + 2 levels deep\n",
        "    include_external=False,    # Stay within the same domain\n",
        "    max_pages=50,              # Maximum number of pages to crawl (optional)\n",
        "    score_threshold=0.3,       # Minimum score for URLs to be crawled (optional)\n",
        ")"
      ],
      "metadata": {
        "id": "UyXy8aFi3yMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install crawl4ai"
      ],
      "metadata": {
        "id": "_KTSp1Yu8TSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!crawl4ai-setup"
      ],
      "metadata": {
        "id": "nC2r8NWe8xB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n",
        "from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy\n",
        "from crawl4ai.deep_crawling import BestFirstCrawlingStrategy\n",
        "from crawl4ai.deep_crawling import BFSDeepCrawlStrategy\n",
        "from crawl4ai.deep_crawling.filters import (\n",
        "    FilterChain,\n",
        "    DomainFilter,\n",
        "    URLPatternFilter,\n",
        "    ContentTypeFilter\n",
        ")\n",
        "from crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\n",
        "\n",
        "async def run_advanced_crawler():\n",
        "    # Create a sophisticated filter chain\n",
        "    filter_chain = FilterChain([\n",
        "        # Domain boundaries\n",
        "        DomainFilter(\n",
        "            allowed_domains=[\"jobright.ai\"]\n",
        "            # blocked_domains=[\"old.docs.example.com\"]\n",
        "        ),\n",
        "\n",
        "        # URL patterns to include\n",
        "        # URLPatternFilter(patterns=[\"*guide*\", \"*tutorial*\", \"*blog*\"]),\n",
        "\n",
        "        # Content type filtering\n",
        "        ContentTypeFilter(allowed_types=[\"text/html\"])\n",
        "    ])\n",
        "\n",
        "\n",
        "    # Set up the configuration\n",
        "    config = CrawlerRunConfig(\n",
        "        deep_crawl_strategy =BFSDeepCrawlStrategy()\n",
        "\n",
        "\n",
        "        deep_crawl_strategy=BestFirstCrawlingStrategy(\n",
        "            max_depth=2,\n",
        "            include_external=False,\n",
        "            filter_chain=filter_chain\n",
        "        ),\n",
        "        scraping_strategy=LXMLWebScrapingStrategy(),\n",
        "        stream=True,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Execute the crawl\n",
        "    results = []\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        async for result in await crawler.arun(\"https://jobright.ai/jobs/info\", config=config):\n",
        "            results.append(result)\n",
        "            score = result.metadata.get(\"score\", 0)\n",
        "            depth = result.metadata.get(\"depth\", 0)\n",
        "            print(f\"Depth: {depth} | Score: {score:.2f} | {result.url}\")\n",
        "\n",
        "    # Analyze the results\n",
        "    print(f\"Crawled {len(results)} high-value pages\")\n",
        "    print(f\"Average score: {sum(r.metadata.get('score', 0) for r in results) / len(results):.2f}\")\n",
        "\n",
        "    # Group by depth\n",
        "    depth_counts = {}\n",
        "    for result in results:\n",
        "        depth = result.metadata.get(\"depth\", 0)\n",
        "        depth_counts[depth] = depth_counts.get(depth, 0) + 1\n",
        "\n",
        "    print(\"Pages crawled by depth:\")\n",
        "    for depth, count in sorted(depth_counts.items()):\n",
        "        print(f\"  Depth {depth}: {count} pages\")\n",
        "    return results\n",
        "if __name__ == \"__main__\":\n",
        "    results = await run_advanced_crawler()\n"
      ],
      "metadata": {
        "id": "aWeCA2m97xHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_url =''\n",
        "end_word_categorys = []\n",
        "parmeters = []\n",
        "result_urls = []\n",
        "for name in end_categorys:\n",
        "  for para in parmeters:\n",
        "    url = prepare_url(base_url,name,para)\n",
        "    result_urls = get_all_jobpost_urls(url)\n",
        "    save_to_db(result_urls)\n",
        "\n"
      ],
      "metadata": {
        "id": "PD_2GtYIB-dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scrapy"
      ],
      "metadata": {
        "id": "0wpppVN9cPA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "from scrapy.spiders import CrawlSpider, Rule\n",
        "from scrapy.linkextractors import LinkExtractor\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from scrapy.utils.project import get_project_settings # Useful for default settings\n",
        "\n",
        "# This is often needed if running in Jupyter/IPython to avoid \"ReactorNotRestartable\"\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- Your Spider Definition ---\n",
        "class AllLinksSpider(CrawlSpider):\n",
        "    name = 'all_links_scraper_cell' # Changed name slightly to avoid clashes if you have the other one\n",
        "    allowed_domains = ['jobright.ai']\n",
        "    # start_urls = ['https://jobright.ai/jobs/backenddeveloper/']\n",
        "    start_urls = ['https://jobright.ai/jobs/backenddeveloper'] # More general starting point\n",
        "    # Counter for processed URLs or items\n",
        "    processed_url_count = 0\n",
        "    MAX_URLS_TO_PROCESS = 100\n",
        "    # Custom settings for this spider if needed (can also be passed to CrawlerProcess)\n",
        "    custom_settings = {\n",
        "        'LOG_LEVEL': 'INFO', # 'DEBUG' for more verbosity\n",
        "        'DOWNLOAD_DELAY': 0.25,  # Be respectful\n",
        "        'CONCURRENT_REQUESTS_PER_DOMAIN': 8,\n",
        "        # 'DEPTH_LIMIT': 2 # Uncomment to limit crawl depth\n",
        "    }\n",
        "\n",
        "    rules = (\n",
        "        Rule(\n",
        "            LinkExtractor(\n",
        "                allow_domains=['jobright.ai'],\n",
        "                deny=(\n",
        "                    r'/login', r'/register', r'/password', # Example patterns to avoid\n",
        "                    r'mailto:', r'tel:', # Avoid mail and tel links\n",
        "                )\n",
        "            ),\n",
        "            callback='parse_page_links',\n",
        "            follow=True\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AllLinksSpider, self).__init__(*args, **kwargs)\n",
        "        # Get the list passed from the CrawlerProcess or default to an empty list\n",
        "        self.collected_links_list = kwargs.get('output_list', [])\n",
        "\n",
        "    def parse_page_links(self, response):\n",
        "        self.logger.info(f\"Processing page: {response.url}\")\n",
        "        links_on_this_page = response.css('a::attr(href)').getall()\n",
        "\n",
        "        for link_href in links_on_this_page:\n",
        "            absolute_link = response.urljoin(link_href)\n",
        "            # Check if it's within allowed domains again, just to be safe if LinkExtractor somehow missed\n",
        "            if self.allowed_domains and any(domain in scrapy.utils.url.get_domain(absolute_link) for domain in self.allowed_domains):\n",
        "                link_data = {\n",
        "                    'source_page_url': response.url,\n",
        "                    'extracted_link': absolute_link\n",
        "                }\n",
        "                # Append to the list provided during initialization\n",
        "                self.collected_links_list.append(link_data)\n",
        "                # Still yield if you want to use Scrapy's feed exporters or other pipelines\n",
        "                yield link_data"
      ],
      "metadata": {
        "id": "JC0b2n4ycLDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This list will be populated by the spider\n",
        "scraped_links_data = []\n",
        "\n",
        "# --- Configure and Run the Crawler ---\n",
        "# Get project settings if you have a settings.py, otherwise, it provides defaults\n",
        "settings = get_project_settings()\n",
        "\n",
        "# Override or add settings\n",
        "settings.set('USER_AGENT', 'MyCustomBot/1.0 (+http://mywebsite.com/botinfo)')\n",
        "# If you want to output to a file as well using Scrapy's feed exporters:\n",
        "# settings.set('FEEDS', {\n",
        "#     'output_links.json': {'format': 'json', 'overwrite': True},\n",
        "# })\n",
        "\n",
        "# Create a CrawlerProcess\n",
        "# The 'settings' argument can be a Settings object or a dictionary\n",
        "process = CrawlerProcess(settings=settings)\n",
        "\n",
        "# Pass the list to the spider instance when scheduling it\n",
        "# The spider's __init__ will pick up 'output_list' from kwargs\n",
        "process.crawl(AllLinksSpider, output_list=scraped_links_data)\n",
        "\n",
        "# The script will block here until all crawling is finished\n",
        "print(\"Starting Scrapy process...\")\n",
        "process.start()\n",
        "print(\"Scrapy process finished.\")\n",
        "\n",
        "# --- Now print the collected links ---\n",
        "print(f\"\\n--- Collected {len(scraped_links_data)} link entries: ---\")\n",
        "unique_extracted_links = set()\n",
        "for item in scraped_links_data:\n",
        "    print(f\"From: {item['source_page_url']} -> Found: {item['extracted_link']}\")\n",
        "    unique_extracted_links.add(item['extracted_link'])\n",
        "\n",
        "print(f\"\\n--- {len(unique_extracted_links)} Unique Extracted Links: ---\")\n",
        "for link in sorted(list(unique_extracted_links)): # Print sorted unique links\n",
        "    print(link)\n",
        "\n",
        "# If you want just a flat list of the unique extracted URLs:\n",
        "final_unique_links_list = sorted(list(unique_extracted_links))\n",
        "# print(\"\\nFinal flat list of unique links:\")\n",
        "# print(final_unique_links_list)"
      ],
      "metadata": {
        "id": "p2OIReYAgzK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# extract jobposts from jobright through api like url"
      ],
      "metadata": {
        "id": "koq1K5P4MeI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# categorys in json\n"
      ],
      "metadata": {
        "id": "-kOcpZFFwvpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category = {\n",
        "  \"job_category\": {\n",
        "    \"Software & IT\": [\n",
        "      \"Backend Engineer\",\n",
        "      \"Java Engineer\",\n",
        "      \"Python Engineer\",\n",
        "      \".Net Engineer\",\n",
        "      \"C/C++ Engineer\",\n",
        "      \"Golang Engineer\",\n",
        "      \"Full Stack Engineer\",\n",
        "      \"Blockchain Engineer\",\n",
        "      \"Salesforce Developer\",\n",
        "      \"Frontend Software Engineer\",\n",
        "      \"React Developer\",\n",
        "      \"UI/UX Developer\",\n",
        "      \"iOS/Swift Developer\",\n",
        "      \"Android Developer\",\n",
        "      \"Flutter Developer\",\n",
        "      \"Unity Developer\",\n",
        "      \"Unreal Engine Developer\",\n",
        "      \"AR/VR Developer\",\n",
        "      \"Game Developer\",\n",
        "      \"Software Testing/Quality Assurance Engineer\",\n",
        "      \"Automation Test Engineer\",\n",
        "      \"QA Manager\",\n",
        "      \"Network Security Engineer\",\n",
        "      \"Cloud Security Engineer\",\n",
        "      \"Cyber Security Analyst\",\n",
        "      \"Cyber Security Engineer\",\n",
        "      \"Network Engineer\",\n",
        "      \"Systems Engineer\",\n",
        "      \"Site Reliability Engineer (SRE)\",\n",
        "      \"DevOps\",\n",
        "      \"SoC Analyst\",\n",
        "      \"IT Support Specialist\",\n",
        "      \"Help Desk Technician/Desktop Support Technician\",\n",
        "      \"System Administrator\",\n",
        "      \"Network Support Specialist\",\n",
        "      \"Salesforce Administrator\",\n",
        "      \"Database Administrator\",\n",
        "      \"Machine Learning Engineer\",\n",
        "      \"AI Engineer\",\n",
        "      \"LLM Engineer\",\n",
        "      \"Machine Learning/AI Researcher\",\n",
        "      \"Machine Learning, Deep Learning\",\n",
        "      \"Machine Learning, Model Training and Inference\",\n",
        "      \"Machine Learning, Search System\",\n",
        "      \"Machine Learning, Ads\",\n",
        "      \"Machine Learning, Operations (ML Ops)\",\n",
        "      \"Machine Learning, Infrastructure\",\n",
        "      \"Machine Learning, Computer Vision\",\n",
        "      \"Data Annotation/AI Tutor\",\n",
        "      \"Sales Engineer\",\n",
        "      \"Developer Relations\",\n",
        "      \"Solutions Architect\",\n",
        "      \"Technical Writing\",\n",
        "      \"Data Analyst\",\n",
        "      \"Data Scientist\",\n",
        "      \"Data Engineer\",\n",
        "      \"ETL Developer\",\n",
        "      \"Data Warehouse Engineer\",\n",
        "      \"Business/BI Analyst\",\n",
        "      \"Power BI Developer\",\n",
        "      \"Engineering Manager\",\n",
        "      \"Software Architect\",\n",
        "      \"Engineering Director/VP\",\n",
        "      \"CTO\",\n",
        "      \"Project/Program Manager\",\n",
        "      \"Technical Project Manager\",\n",
        "      \"Scrum Master\"\n",
        "    ],\n",
        "    \"Hardware & Electrical Engineering\": [\n",
        "      \"Electronics Engineer\",\n",
        "      \"Hardware Engineer\",\n",
        "      \"Embedded Software Engineer\",\n",
        "      \"ASIC Engineer\",\n",
        "      \"FPGA Engineer\",\n",
        "      \"RF (Radio Frequency) Engineer\",\n",
        "      \"PCB Engineer\",\n",
        "      \"Systems Integration Engineer\",\n",
        "      \"IC Design Engineer\",\n",
        "      \"Digital IC Verification Engineer\",\n",
        "      \"Analog IC Design Engineer\",\n",
        "      \"Electrical Engineer\",\n",
        "      \"Automation Engineer\",\n",
        "      \"Electromechanical Engineer\",\n",
        "      \"Robotics Engineer\",\n",
        "      \"Controls Engineer\",\n",
        "      \"Electrical Test Engineer\",\n",
        "      \"Hardware Test Engineer\",\n",
        "      \"Project/Program Manager\",\n",
        "      \"Telecommunications Engineer\",\n",
        "      \"Network Engineer\",\n",
        "      \"Wireless/Antenna Engineer\",\n",
        "      \"Battery Engineer\",\n",
        "      \"Motor Engineer\",\n",
        "      \"Aerospace Engineer\",\n",
        "      \"Sales Engineer\",\n",
        "      \"Solutions Architect\"\n",
        "    ],\n",
        "    \"Mechanical & Industrial Engineering\": [\n",
        "      \"Mechanical Engineer\",\n",
        "      \"Manufacturing Engineer\",\n",
        "      \"Process Engineer\",\n",
        "      \"Industrial Engineer\",\n",
        "      \"Mechatronics Engineer\",\n",
        "      \"Operations Manager/Director\",\n",
        "      \"Safety Engineer\",\n",
        "      \"Chemical Engineer\",\n",
        "      \"Laboratory Technician\",\n",
        "      \"Automotive Engineer\",\n",
        "      \"Powertrain Engineer\",\n",
        "      \"Autonomous Driving System Engineer\",\n",
        "      \"Quality Assurance Specialist\",\n",
        "      \"EHS (Environment, Health, Safety) Engineer\",\n",
        "      \"Project/Program Manager\"\n",
        "    ],\n",
        "    \"Product Management\": [\n",
        "      \"Product Analyst\",\n",
        "      \"Product Manager\",\n",
        "      \"Technical Product Manager\",\n",
        "      \"Product Manager, Consumer Software\",\n",
        "      \"Product Manager, B2B/SaaS\",\n",
        "      \"Product Manager, Hardware/Robotics/IoT\",\n",
        "      \"AI Product Manager\",\n",
        "      \"Game Designer\"\n",
        "    ],\n",
        "    \"Customer Service & Success\": [\n",
        "      \"Customer Service Representative\",\n",
        "      \"Customer Service Manager\",\n",
        "      \"Customer Support\",\n",
        "      \"Customer Success\"\n",
        "    ],\n",
        "    \"Sales\": [\n",
        "      \"Sales Development Representative\",\n",
        "      \"Inside Sales Representative\",\n",
        "      \"Account Executive, SMB\",\n",
        "      \"Field Sales Representative\",\n",
        "      \"Enterprise Sales\",\n",
        "      \"Channel Sales\",\n",
        "      \"Business Development\",\n",
        "      \"Partnership\",\n",
        "      \"Sales Manager\",\n",
        "      \"Regional Sales Manager\",\n",
        "      \"Sales Director/VP\",\n",
        "      \"Automotive Sales\",\n",
        "      \"Real Estate Sales\",\n",
        "      \"Leasing Manager\",\n",
        "      \"Retail Sales\",\n",
        "      \"Store Manager\",\n",
        "      \"Medical Sales\",\n",
        "      \"Medical Device Sales\",\n",
        "      \"Financial Advisor\",\n",
        "      \"Insurance Sales\",\n",
        "      \"Sales Support\",\n",
        "      \"Sales Operations Specialist\"\n",
        "    ],\n",
        "    \"HR, Admin & Legal\": [\n",
        "      \"Human Resource Specialist\",\n",
        "      \"Recruiter/Sourcer\",\n",
        "      \"Recruiting Coordinator\",\n",
        "      \"Payroll Specialist\",\n",
        "      \"HR Business Partner\",\n",
        "      \"Human Resource Manager/Director\",\n",
        "      \"Administrative Assistant\",\n",
        "      \"Executive Assistant\",\n",
        "      \"Chief of Staff\",\n",
        "      \"Office Manager\",\n",
        "      \"Receptionist\",\n",
        "      \"Data Entry Clerk\",\n",
        "      \"Corporate Counsel\",\n",
        "      \"Paralegal\",\n",
        "      \"Legal Assistant\",\n",
        "      \"Litigation Lawyer\",\n",
        "      \"Intellectual Property Lawyer\",\n",
        "      \"Criminal Lawyer\",\n",
        "      \"Family Lawyer\",\n",
        "      \"Immigration Lawyer\",\n",
        "      \"Compliance Specialist\",\n",
        "      \"Risk Analyst\",\n",
        "      \"Court Clerk\",\n",
        "      \"Case Manager\",\n",
        "      \"Legal Operations Manager\"\n",
        "    ],\n",
        "    \"Finance & Accounting\": [\n",
        "      \"Accountant\",\n",
        "      \"Controller\",\n",
        "      \"Tax Specialist\",\n",
        "      \"Auditor\",\n",
        "      \"Corporate Finance Analyst\",\n",
        "      \"Treasury\",\n",
        "      \"Financial Analyst\",\n",
        "      \"Risk Analyst\",\n",
        "      \"Securities Trader\",\n",
        "      \"Quantitative Analyst/Researcher\",\n",
        "      \"Investment Manager\",\n",
        "      \"Equity Analyst\",\n",
        "      \"Asset Manager\",\n",
        "      \"Portfolio Manager\",\n",
        "      \"Commercial Banker\",\n",
        "      \"Investment Banker\",\n",
        "      \"Credit Analyst\",\n",
        "      \"Loan Officer\",\n",
        "      \"Investment Analyst/Associate\",\n",
        "      \"Investment Direct/VP\",\n",
        "      \"Investment Partner\",\n",
        "      \"Portfolio Operations Manager\",\n",
        "      \"Fundraising Manager\",\n",
        "      \"Investor Relations Manager\",\n",
        "      \"Actuary\",\n",
        "      \"Underwriter\"\n",
        "    ],\n",
        "    \"Design & Creative\": [\n",
        "      \"Graphic Designer\",\n",
        "      \"UI Designer\",\n",
        "      \"UX Designer\",\n",
        "      \"UX Researcher\",\n",
        "      \"3D Designer\",\n",
        "      \"Animator\",\n",
        "      \"Illustrator\",\n",
        "      \"Video Editor\",\n",
        "      \"Creative/Art Director\",\n",
        "      \"Motion Designer\",\n",
        "      \"Interior Designer\",\n",
        "      \"Landscape Designer\",\n",
        "      \"Industrial Designer\"\n",
        "    ],\n",
        "    \"Real Estate & Construction\": [\n",
        "      \"Leasing Consultant\",\n",
        "      \"Property Manager\",\n",
        "      \"Architect\",\n",
        "      \"Landscape Architect\",\n",
        "      \"Urban Planner\",\n",
        "      \"Construction Project Manager\",\n",
        "      \"Civil Engineer\",\n",
        "      \"Structural Engineer\"\n",
        "    ],\n",
        "    \"Marketing & Communications\": [\n",
        "      \"Content Marketing/Strategy\",\n",
        "      \"SEO\",\n",
        "      \"Social Media Management\",\n",
        "      \"Copywriter\",\n",
        "      \"Product Marketing\",\n",
        "      \"Brand Manager\",\n",
        "      \"Public Relations\",\n",
        "      \"Community Manager\",\n",
        "      \"Event Marketing Specialist\",\n",
        "      \"Growth Marketing\",\n",
        "      \"Advertising Specialist\",\n",
        "      \"Performance Marketing\",\n",
        "      \"Lifecycle Marketing\",\n",
        "      \"Email Marketing\"\n",
        "    ],\n",
        "    \"Supply Chain & Operations\": [\n",
        "      \"Supply Chain Manager\",\n",
        "      \"Inventory Manager\",\n",
        "      \"Logistics Manager\",\n",
        "      \"Warehouse Manager\",\n",
        "      \"Distribution Center Manager\",\n",
        "      \"Procurement Manager\",\n",
        "      \"Facilities Manager\"\n",
        "    ],\n",
        "    \"Consulting\": [\n",
        "      \"IT Consultant\",\n",
        "      \"Business Analyst\",\n",
        "      \"Data Consultant\",\n",
        "      \"Cyber Security Consultant\",\n",
        "      \"Business Strategy Consultant\",\n",
        "      \"Market Research Analyst\",\n",
        "      \"Change Management Consultant\",\n",
        "      \"Operations Consultant\",\n",
        "      \"Financial Consultant\",\n",
        "      \"Risk Management Consultant\",\n",
        "      \"Mergers & Acquisitions (M&A) Consultant\"\n",
        "    ],\n",
        "    \"Energy & Environmental\": [\n",
        "      \"Energy Engineer\",\n",
        "      \"Renewable Energy Engineer\",\n",
        "      \"Nuclear Engineer\",\n",
        "      \"Power Systems Engineer\",\n",
        "      \"Environmental Engineer\",\n",
        "      \"Environmental Scientist\"\n",
        "    ],\n",
        "    \"Education & Training\": [\n",
        "      \"K-12 Teaching\",\n",
        "      \"Higher Education Teaching\",\n",
        "      \"Corporate Training and Development\",\n",
        "      \"Educational Administration\",\n",
        "      \"Academic Dean\"\n",
        "    ],\n",
        "    \"Healthcare & Life Sciences\": [\n",
        "      \"Healthcare Data Analyst\",\n",
        "      \"Healthcare Data Scientist\",\n",
        "      \"Healthcare IT Specialist\",\n",
        "      \"EHR (Electronic Health Records) System Administrator\",\n",
        "      \"Biomedical Engineer\",\n",
        "      \"Clinical Engineer\",\n",
        "      \"Biomedical Equipment Technician\",\n",
        "      \"Biologist\",\n",
        "      \"Pharmacologist\",\n",
        "      \"Chemist\",\n",
        "      \"Biochemist\",\n",
        "      \"Formulation Scientist\",\n",
        "      \"Toxicologist\",\n",
        "      \"DMPK Scientist\",\n",
        "      \"Clinical Research Scientist\",\n",
        "      \"Clinical Research Associate\",\n",
        "      \"Biostatistician\",\n",
        "      \"Regulatory Affairs Specialist\",\n",
        "      \"Medical Writer\",\n",
        "      \"Health Product Manager\",\n",
        "      \"Clinical Operations Manager\",\n",
        "      \"Healthcare Compliance Manager\",\n",
        "      \"Healthcare Quality Improvement Specialist\"\n",
        "    ],\n",
        "    \"Government & Non-Profit\": [\n",
        "      \"Government Relations Manager\",\n",
        "      \"Policy Analyst\",\n",
        "      \"Program Manager\",\n",
        "      \"Fundraising Coordinator\",\n",
        "      \"Volunteer Coordinator\"\n",
        "    ]\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "T1jlpQAuDjkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cookies = {\n",
        "    '_hjSessionUser_6388958': 'eyJpZCI6IjQxOGUwNzcwLWE3YzAtNTBmOC05NDE4LTAxYjQwNzFkNDYwZiIsImNyZWF0ZWQiOjE3NTA4Mjk4MjE5MTQsImV4aXN0aW5nIjpmYWxzZX0=',\n",
        "    '_hjSession_6388958': 'eyJpZCI6IjYzNDlmYTVmLTlkNTEtNDU2YS04MDFiLWEwNTljNTRiYmY2NyIsImMiOjE3NTA4Mjk4MjE5MTUsInMiOjAsInIiOjAsInNiIjowLCJzciI6MCwic2UiOjAsImZzIjoxfQ==',\n",
        "    '_uetsid': '6b81f720518611f0981ea504cec8251d',\n",
        "    '_uetvid': '6b823070518611f09fcff1af0cf89f08',\n",
        "    '_gcl_au': '1.1.1167952771.1750829822',\n",
        "    '_clck': 'w0flwz%7C2%7Cfx2%7C0%7C2002',\n",
        "    '_ga': 'GA1.1.1812497396.1750829822',\n",
        "    '_ga_ETKKWETCJD': 'GS2.1.s1750829822$o1$g0$t1750829822$j60$l0$h928065022',\n",
        "    '_tt_enable_cookie': '1',\n",
        "    '_ttp': '01JYJSDX1B6KS3XCK7GN8QCHB8_.tt.1',\n",
        "    'ttcsid': '1750829823024::gtewOAeumkry7EaMvHXD.1.1750829823024',\n",
        "    '_clsk': 'wpanqj%7C1750829823973%7C1%7C1%7Cb.clarity.ms%2Fcollect',\n",
        "    'ttcsid_CM0IJ53C77U0797CAP10': '1750829823023::jSB6o9Ve1MM-ssaV1TPo.1.1750829823975',\n",
        "}"
      ],
      "metadata": {
        "id": "8HCmdFNZwnv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# send request to get jobposts"
      ],
      "metadata": {
        "id": "ddjU4rjSw8sR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "\n",
        "num_of_results = 200\n",
        "job_title = 'Backend Engineer'\n",
        "\n",
        "def get_job_posts(job_title,num_of_results):\n",
        "  headers = {\n",
        "      'accept': 'application/json, text/plain, */*',\n",
        "      'accept-language': 'en-US,en;q=0.9,en-IN;q=0.8',\n",
        "      'baggage': 'sentry-environment=production,sentry-release=pigeon_production%40v0.0.819,sentry-public_key=5f46138160b2461b9e0fb4bb1cc803bc,sentry-trace_id=81177d03883d49d0861aa611e3092220,sentry-sample_rate=0.01,sentry-transaction=%2Fjobs%2F%5Bvisit%5D,sentry-sampled=false',\n",
        "      'content-type': 'application/json',\n",
        "      'dnt': '1',\n",
        "      'origin': 'https://jobright.ai',\n",
        "      'priority': 'u=1, i',\n",
        "      'referer': 'https://jobright.ai/jobs/back',\n",
        "      'sec-ch-ua': '\"Not)A;Brand\";v=\"8\", \"Chromium\";v=\"138\", \"Microsoft Edge\";v=\"138\"',\n",
        "      'sec-ch-ua-mobile': '?0',\n",
        "      'sec-ch-ua-platform': '\"Windows\"',\n",
        "      'sec-fetch-dest': 'empty',\n",
        "      'sec-fetch-mode': 'cors',\n",
        "      'sec-fetch-site': 'same-origin',\n",
        "      'sentry-trace': '81177d03883d49d0861aa611e3092220-b7f6993eb0a7f9ee-0',\n",
        "      'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0',\n",
        "      'x-client-type': 'web',\n",
        "      # 'cookie': '_hjSessionUser_6388958=eyJpZCI6IjQxOGUwNzcwLWE3YzAtNTBmOC05NDE4LTAxYjQwNzFkNDYwZiIsImNyZWF0ZWQiOjE3NTA4Mjk4MjE5MTQsImV4aXN0aW5nIjpmYWxzZX0=; _hjSession_6388958=eyJpZCI6IjYzNDlmYTVmLTlkNTEtNDU2YS04MDFiLWEwNTljNTRiYmY2NyIsImMiOjE3NTA4Mjk4MjE5MTUsInMiOjAsInIiOjAsInNiIjowLCJzciI6MCwic2UiOjAsImZzIjoxfQ==; _uetsid=6b81f720518611f0981ea504cec8251d; _uetvid=6b823070518611f09fcff1af0cf89f08; _gcl_au=1.1.1167952771.1750829822; _clck=w0flwz%7C2%7Cfx2%7C0%7C2002; _ga=GA1.1.1812497396.1750829822; _ga_ETKKWETCJD=GS2.1.s1750829822$o1$g0$t1750829822$j60$l0$h928065022; _tt_enable_cookie=1; _ttp=01JYJSDX1B6KS3XCK7GN8QCHB8_.tt.1; ttcsid=1750829823024::gtewOAeumkry7EaMvHXD.1.1750829823024; _clsk=wpanqj%7C1750829823973%7C1%7C1%7Cb.clarity.ms%2Fcollect; ttcsid_CM0IJ53C77U0797CAP10=1750829823023::jSB6o9Ve1MM-ssaV1TPo.1.1750829823975',\n",
        "  }\n",
        "\n",
        "  params = {\n",
        "      'sortCondition': '0',\n",
        "      'count': str(num_of_results),\n",
        "      'position': '0',\n",
        "  }\n",
        "\n",
        "  json_data = {\n",
        "      'jobTitle': job_title,\n",
        "      'city': 'Within US',\n",
        "      'jobTypes': [1,2,3,4,],\n",
        "      'seniority': [1,2,3,4,5,6,],\n",
        "      'workModel': [1,2,3,],\n",
        "      'radiusRange': 1000,\n",
        "      'position': 0,\n",
        "      'count': 1000,\n",
        "  }\n",
        "\n",
        "  response = requests.post(\n",
        "      'https://jobright.ai/swan/recommend/visitor-list/jobs',\n",
        "      params=params,\n",
        "      # cookies=cookies,\n",
        "      headers=headers,\n",
        "      json=json_data,\n",
        "  )\n",
        "  print(response)\n",
        "  return response\n"
      ],
      "metadata": {
        "id": "gBvKyxa5nI0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main code"
      ],
      "metadata": {
        "id": "A_ue4_q9yTXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "job_cate_list = []\n",
        "for key,value in category['job_category'].items():\n",
        "    job_cate_list.extend(category['job_category'][key])\n",
        "print(job_cate_list)\n",
        "# data = dict(category['job_category'])\n",
        "# data.values()\n",
        "print(len(job_cate_list))"
      ],
      "metadata": {
        "id": "Zv5OMAhYDvHM",
        "outputId": "915596bd-1d95-494e-a9c0-348b013eedb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Backend Engineer', 'Java Engineer', 'Python Engineer', '.Net Engineer', 'C/C++ Engineer', 'Golang Engineer', 'Full Stack Engineer', 'Blockchain Engineer', 'Salesforce Developer', 'Frontend Software Engineer', 'React Developer', 'UI/UX Developer', 'iOS/Swift Developer', 'Android Developer', 'Flutter Developer', 'Unity Developer', 'Unreal Engine Developer', 'AR/VR Developer', 'Game Developer', 'Software Testing/Quality Assurance Engineer', 'Automation Test Engineer', 'QA Manager', 'Network Security Engineer', 'Cloud Security Engineer', 'Cyber Security Analyst', 'Cyber Security Engineer', 'Network Engineer', 'Systems Engineer', 'Site Reliability Engineer (SRE)', 'DevOps', 'SoC Analyst', 'IT Support Specialist', 'Help Desk Technician/Desktop Support Technician', 'System Administrator', 'Network Support Specialist', 'Salesforce Administrator', 'Database Administrator', 'Machine Learning Engineer', 'AI Engineer', 'LLM Engineer', 'Machine Learning/AI Researcher', 'Machine Learning, Deep Learning', 'Machine Learning, Model Training and Inference', 'Machine Learning, Search System', 'Machine Learning, Ads', 'Machine Learning, Operations (ML Ops)', 'Machine Learning, Infrastructure', 'Machine Learning, Computer Vision', 'Data Annotation/AI Tutor', 'Sales Engineer', 'Developer Relations', 'Solutions Architect', 'Technical Writing', 'Data Analyst', 'Data Scientist', 'Data Engineer', 'ETL Developer', 'Data Warehouse Engineer', 'Business/BI Analyst', 'Power BI Developer', 'Engineering Manager', 'Software Architect', 'Engineering Director/VP', 'CTO', 'Project/Program Manager', 'Technical Project Manager', 'Scrum Master', 'Electronics Engineer', 'Hardware Engineer', 'Embedded Software Engineer', 'ASIC Engineer', 'FPGA Engineer', 'RF (Radio Frequency) Engineer', 'PCB Engineer', 'Systems Integration Engineer', 'IC Design Engineer', 'Digital IC Verification Engineer', 'Analog IC Design Engineer', 'Electrical Engineer', 'Automation Engineer', 'Electromechanical Engineer', 'Robotics Engineer', 'Controls Engineer', 'Electrical Test Engineer', 'Hardware Test Engineer', 'Project/Program Manager', 'Telecommunications Engineer', 'Network Engineer', 'Wireless/Antenna Engineer', 'Battery Engineer', 'Motor Engineer', 'Aerospace Engineer', 'Sales Engineer', 'Solutions Architect', 'Mechanical Engineer', 'Manufacturing Engineer', 'Process Engineer', 'Industrial Engineer', 'Mechatronics Engineer', 'Operations Manager/Director', 'Safety Engineer', 'Chemical Engineer', 'Laboratory Technician', 'Automotive Engineer', 'Powertrain Engineer', 'Autonomous Driving System Engineer', 'Quality Assurance Specialist', 'EHS (Environment, Health, Safety) Engineer', 'Project/Program Manager', 'Product Analyst', 'Product Manager', 'Technical Product Manager', 'Product Manager, Consumer Software', 'Product Manager, B2B/SaaS', 'Product Manager, Hardware/Robotics/IoT', 'AI Product Manager', 'Game Designer', 'Customer Service Representative', 'Customer Service Manager', 'Customer Support', 'Customer Success', 'Sales Development Representative', 'Inside Sales Representative', 'Account Executive, SMB', 'Field Sales Representative', 'Enterprise Sales', 'Channel Sales', 'Business Development', 'Partnership', 'Sales Manager', 'Regional Sales Manager', 'Sales Director/VP', 'Automotive Sales', 'Real Estate Sales', 'Leasing Manager', 'Retail Sales', 'Store Manager', 'Medical Sales', 'Medical Device Sales', 'Financial Advisor', 'Insurance Sales', 'Sales Support', 'Sales Operations Specialist', 'Human Resource Specialist', 'Recruiter/Sourcer', 'Recruiting Coordinator', 'Payroll Specialist', 'HR Business Partner', 'Human Resource Manager/Director', 'Administrative Assistant', 'Executive Assistant', 'Chief of Staff', 'Office Manager', 'Receptionist', 'Data Entry Clerk', 'Corporate Counsel', 'Paralegal', 'Legal Assistant', 'Litigation Lawyer', 'Intellectual Property Lawyer', 'Criminal Lawyer', 'Family Lawyer', 'Immigration Lawyer', 'Compliance Specialist', 'Risk Analyst', 'Court Clerk', 'Case Manager', 'Legal Operations Manager', 'Accountant', 'Controller', 'Tax Specialist', 'Auditor', 'Corporate Finance Analyst', 'Treasury', 'Financial Analyst', 'Risk Analyst', 'Securities Trader', 'Quantitative Analyst/Researcher', 'Investment Manager', 'Equity Analyst', 'Asset Manager', 'Portfolio Manager', 'Commercial Banker', 'Investment Banker', 'Credit Analyst', 'Loan Officer', 'Investment Analyst/Associate', 'Investment Direct/VP', 'Investment Partner', 'Portfolio Operations Manager', 'Fundraising Manager', 'Investor Relations Manager', 'Actuary', 'Underwriter', 'Graphic Designer', 'UI Designer', 'UX Designer', 'UX Researcher', '3D Designer', 'Animator', 'Illustrator', 'Video Editor', 'Creative/Art Director', 'Motion Designer', 'Interior Designer', 'Landscape Designer', 'Industrial Designer', 'Leasing Consultant', 'Property Manager', 'Architect', 'Landscape Architect', 'Urban Planner', 'Construction Project Manager', 'Civil Engineer', 'Structural Engineer', 'Content Marketing/Strategy', 'SEO', 'Social Media Management', 'Copywriter', 'Product Marketing', 'Brand Manager', 'Public Relations', 'Community Manager', 'Event Marketing Specialist', 'Growth Marketing', 'Advertising Specialist', 'Performance Marketing', 'Lifecycle Marketing', 'Email Marketing', 'Supply Chain Manager', 'Inventory Manager', 'Logistics Manager', 'Warehouse Manager', 'Distribution Center Manager', 'Procurement Manager', 'Facilities Manager', 'IT Consultant', 'Business Analyst', 'Data Consultant', 'Cyber Security Consultant', 'Business Strategy Consultant', 'Market Research Analyst', 'Change Management Consultant', 'Operations Consultant', 'Financial Consultant', 'Risk Management Consultant', 'Mergers & Acquisitions (M&A) Consultant', 'Energy Engineer', 'Renewable Energy Engineer', 'Nuclear Engineer', 'Power Systems Engineer', 'Environmental Engineer', 'Environmental Scientist', 'K-12 Teaching', 'Higher Education Teaching', 'Corporate Training and Development', 'Educational Administration', 'Academic Dean', 'Healthcare Data Analyst', 'Healthcare Data Scientist', 'Healthcare IT Specialist', 'EHR (Electronic Health Records) System Administrator', 'Biomedical Engineer', 'Clinical Engineer', 'Biomedical Equipment Technician', 'Biologist', 'Pharmacologist', 'Chemist', 'Biochemist', 'Formulation Scientist', 'Toxicologist', 'DMPK Scientist', 'Clinical Research Scientist', 'Clinical Research Associate', 'Biostatistician', 'Regulatory Affairs Specialist', 'Medical Writer', 'Health Product Manager', 'Clinical Operations Manager', 'Healthcare Compliance Manager', 'Healthcare Quality Improvement Specialist', 'Government Relations Manager', 'Policy Analyst', 'Program Manager', 'Fundraising Coordinator', 'Volunteer Coordinator']\n",
            "286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for job_title in job_cate_list:\n",
        "  response = get_job_posts(job_title,num_of_results)\n",
        "  time.sleep()"
      ],
      "metadata": {
        "id": "z7hjLw3fyKUu",
        "outputId": "e3b2049e-4ab2-44b4-93d4-60a9482130fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "data = dict(response.json())\n",
        "# print(json.dumps(data))\n",
        "print(len(data['result']['jobList']))\n",
        "job_data = data['result']['jobList']\n",
        "# job_results = data['result']['jobList']['jobResult']\n",
        "# company_results = data['result']['jobList']['companyResult']\n",
        "# job_notes = data['result']['jobList']['jobNotes']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or_A4M2_nQu6",
        "outputId": "720f2a2f-b625-4181-d979-68cf1cfca703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "job_results"
      ],
      "metadata": {
        "id": "V3xx3pWR1hb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "try:\n",
        "  conn = sqlite3.connect('job_posts.db')\n",
        "  c = conn.cursor()\n",
        "\n",
        "  job_results = []\n",
        "  company_results = []\n",
        "  job_notes = []\n",
        "  for job_details in job_data:\n",
        "      post  =job_details['jobResult']\n",
        "      company  =job_details['companyResult']\n",
        "      note  =job_details['jobNotes']\n",
        "      post['impId'] =company['impId'] =note['impId'] = job_details['impId']\n",
        "      job_results.append(post)\n",
        "      company_results.append(company)\n",
        "      job_notes.append(note)\n",
        "  # print(job_results)\n",
        "  df_job_post = pd.DataFrame(job_results)\n",
        "  df_company = pd.DataFrame(company_results)\n",
        "  df_note = pd.DataFrame(job_notes)\n",
        "\n",
        "  dfs = [df_job_post, df_company, df_note]\n",
        "  for data_frames in dfs:\n",
        "    for col in data_frames.columns:\n",
        "      if data_frames[col].apply(type).eq(list).any() or data_frames[col].apply(type).eq(dict).any():\n",
        "          print(f\"Converting list in column '{col}' to JSON string.\")\n",
        "          data_frames[col] = data_frames[col].apply(json.dumps)\n",
        "    # data_frames.drop_duplicates(inplace=True)\n",
        "\n",
        "  df_job_post.to_sql('job_post', conn, if_exists='append', index=False)\n",
        "  df_company.to_sql('company', conn, if_exists='append', index=False)\n",
        "  df_note.to_sql('job_note', conn, if_exists='append', index=False)\n",
        "  print(\"Data successfully written to database.\")\n",
        "\n",
        "finally:\n",
        "  print('closing db')\n",
        "  conn.close()"
      ],
      "metadata": {
        "id": "IvdlZIbJvvPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "try:\n",
        "  conn = sqlite3.connect('job_posts.db')\n",
        "  c = conn.cursor()\n",
        "  df_job_post = pd.read_sql_query(\"SELECT * from job_post\", conn)\n",
        "  df_company = pd.read_sql(\"SELECT * from company\", conn)\n",
        "  df_note = pd.read_sql_query(\"SELECT * from job_note\", conn)\n",
        "finally:\n",
        "  conn.close()\n",
        "\n",
        "df_company"
      ],
      "metadata": {
        "id": "nkR94CzK6UGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import aiohttp\n",
        "from itertools import cycle\n",
        "\n",
        "# --- Configuration ---\n",
        "DB_PATH = 'jobs_database.db'\n",
        "PROXY_TABLE = 'proxies'\n",
        "JOBS_TABLE = 'jobs'\n",
        "RESULTS_TABLE = 'jobs_with_responses'\n",
        "# Set the maximum number of concurrent requests to avoid overloading the server\n",
        "MAX_CONCURRENT_REQUESTS = 5\n",
        "\n",
        "# --- 1. Database Setup (Helper function to create sample data) ---\n",
        "def setup_database():\n",
        "    \"\"\"Creates a sample database with proxies and jobs for demonstration.\"\"\"\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    c = conn.cursor()\n",
        "\n",
        "    # Create proxies table\n",
        "    c.execute(f'''\n",
        "        CREATE TABLE IF NOT EXISTS {PROXY_TABLE} (\n",
        "            proxy_url TEXT PRIMARY KEY,\n",
        "            active INTEGER\n",
        "        )\n",
        "    ''')\n",
        "    proxies = [\n",
        "        # In a real scenario, these would be your actual proxy IPs and ports\n",
        "        # For this example, we use a public proxy testing service.\n",
        "        # If these fail, replace them with other free proxies or your own.\n",
        "        ('http://5.78.69.164:8080', 1), # Example active proxy\n",
        "        ('http://194.135.18.239:5678', 1), # Example active proxy\n",
        "        ('http://1.1.1.1:8080', 0), # Example inactive proxy\n",
        "        ('http://190.61.88.147:8080', 1), # Example active proxy\n",
        "    ]\n",
        "    c.executemany(f'INSERT OR IGNORE INTO {PROXY_TABLE} VALUES (?, ?)', proxies)\n",
        "\n",
        "    # Create jobs table with URLs to scrape\n",
        "    c.execute(f'''\n",
        "        CREATE TABLE IF NOT EXISTS {JOBS_TABLE} (\n",
        "            job_id INTEGER PRIMARY KEY,\n",
        "            url TEXT NOT NULL\n",
        "        )\n",
        "    ''')\n",
        "    urls = [\n",
        "        # Using a site that's good for testing scraping\n",
        "        (1, 'http://httpbin.org/get?job=1'),\n",
        "        (2, 'http://httpbin.org/get?job=2'),\n",
        "        (3, 'http://httpbin.org/status/404'), # This URL will fail\n",
        "        (4, 'http://httpbin.org/get?job=4'),\n",
        "        (5, 'http://httpbin.org/delay/2'), # This URL is slow\n",
        "        (6, 'http://httpbin.org/get?job=6'),\n",
        "        (7, 'http://httpbin.org/get?job=7'),\n",
        "        (8, 'http://httpbin.org/get?job=8'),\n",
        "        (9, 'https://non-existent-domain-12345.com'), # This domain doesn't exist\n",
        "        (10, 'http://httpbin.org/get?job=10'),\n",
        "    ]\n",
        "    c.executemany(f'INSERT OR IGNORE INTO {JOBS_TABLE} VALUES (?, ?)', urls)\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(\"Database setup complete.\")\n",
        "\n",
        "# --- 2. Core Asynchronous Logic ---\n",
        "\n",
        "async def fetch_url(session: aiohttp.ClientSession, url: str, proxy: str):\n",
        "    \"\"\"\n",
        "    Fetches a single URL using a given proxy.\n",
        "    Returns a dictionary with the URL, status, and response text.\n",
        "    \"\"\"\n",
        "    print(f\"Fetching {url} via proxy {proxy}\")\n",
        "    try:\n",
        "        # Set a timeout for the request to avoid getting stuck\n",
        "        timeout = aiohttp.ClientTimeout(total=20)\n",
        "        async with session.get(url, proxy=proxy, timeout=timeout) as response:\n",
        "            return {\n",
        "                \"url\": url,\n",
        "                \"status_code\": response.status,\n",
        "                \"response_text\": await response.text()\n",
        "            }\n",
        "    except Exception as e:\n",
        "        # Catch any exception (timeout, connection error, etc.)\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return {\n",
        "            \"url\": url,\n",
        "            \"status_code\": -1, # Custom code for an application-level error\n",
        "            \"response_text\": str(e)\n",
        "        }\n",
        "\n",
        "async def main():\n",
        "    \"\"\"\n",
        "    Main function to orchestrate the process.\n",
        "    \"\"\"\n",
        "    # a. Read active proxies from the database\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    try:\n",
        "        proxies_df = pd.read_sql_query(f\"SELECT proxy_url FROM {PROXY_TABLE} WHERE active = 1\", conn)\n",
        "        active_proxies = proxies_df['proxy_url'].tolist()\n",
        "        if not active_proxies:\n",
        "            print(\"No active proxies found. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Create a rotator for the proxies\n",
        "        proxy_rotator = cycle(active_proxies)\n",
        "\n",
        "        # b. Read URLs from the database into a DataFrame\n",
        "        jobs_df = pd.read_sql_query(f\"SELECT * FROM {JOBS_TABLE}\", conn)\n",
        "        urls_to_fetch = jobs_df['url'].tolist()\n",
        "\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "    # c. Create a semaphore to limit concurrent requests\n",
        "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
        "\n",
        "    # d. Create a single aiohttp session and create tasks\n",
        "    tasks = []\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        for url in urls_to_fetch:\n",
        "            # This wrapper coroutine acquires the semaphore before fetching\n",
        "            async def fetch_with_semaphore(url, proxy):\n",
        "                async with semaphore:\n",
        "                    return await fetch_url(session, url, proxy)\n",
        "\n",
        "            # Assign a task for each URL, rotating through proxies\n",
        "            task = asyncio.create_task(fetch_with_semaphore(url, next(proxy_rotator)))\n",
        "            tasks.append(task)\n",
        "\n",
        "        # e. Run all tasks concurrently and wait for them to complete\n",
        "        print(f\"Starting {len(tasks)} requests with a concurrency limit of {MAX_CONCURRENT_REQUESTS}...\")\n",
        "        results = await asyncio.gather(*tasks)\n",
        "\n",
        "    # f. Process results and append to the original DataFrame\n",
        "    # Create a mapping from URL to result for efficient lookup\n",
        "    result_map = {res['url']: res for res in results}\n",
        "\n",
        "    jobs_df['status_code'] = jobs_df['url'].map(lambda u: result_map[u]['status_code'])\n",
        "    # Only keep first 500 chars of response to keep DB size reasonable\n",
        "    jobs_df['response_text'] = jobs_df['url'].map(lambda u: result_map[u]['response_text'])\n",
        "\n",
        "    print(\"\\n--- Scraping Results ---\")\n",
        "    print(jobs_df)\n",
        "\n",
        "    # g. Update the database with the new results\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    try:\n",
        "        print(f\"\\nSaving results to table '{RESULTS_TABLE}'...\")\n",
        "        jobs_df.to_sql(RESULTS_TABLE, conn, if_exists='replace', index=False)\n",
        "        print(\"Database updated successfully.\")\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the setup once to create the database\n",
        "    setup_database()\n",
        "\n",
        "    # Run the main asynchronous event loop\n",
        "    await main()"
      ],
      "metadata": {
        "id": "lAZuyrIOG2qF",
        "outputId": "0a1fc1ea-c514-4cbc-8632-9a4c92021232",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database setup complete.\n",
            "Starting 10 requests with a concurrency limit of 5...\n",
            "Fetching http://httpbin.org/get?job=1 via proxy http://5.78.69.164:8080\n",
            "Fetching http://httpbin.org/get?job=2 via proxy http://194.135.18.239:5678\n",
            "Fetching http://httpbin.org/status/404 via proxy http://190.61.88.147:8080\n",
            "Fetching http://httpbin.org/get?job=4 via proxy http://5.78.69.164:8080\n",
            "Fetching http://httpbin.org/delay/2 via proxy http://194.135.18.239:5678\n",
            "Error fetching http://httpbin.org/get?job=2: Cannot connect to host 194.135.18.239:5678 ssl:default [Connect call failed ('194.135.18.239', 5678)]\n",
            "Fetching http://httpbin.org/get?job=6 via proxy http://190.61.88.147:8080\n",
            "Error fetching http://httpbin.org/delay/2: Cannot connect to host 194.135.18.239:5678 ssl:default [Connect call failed ('194.135.18.239', 5678)]\n",
            "Fetching http://httpbin.org/get?job=7 via proxy http://5.78.69.164:8080\n",
            "Error fetching http://httpbin.org/get?job=1: \n",
            "Error fetching http://httpbin.org/status/404: \n",
            "Error fetching http://httpbin.org/get?job=6: \n",
            "Error fetching http://httpbin.org/get?job=7: \n",
            "Error fetching http://httpbin.org/get?job=4: \n",
            "Fetching http://httpbin.org/get?job=8 via proxy http://194.135.18.239:5678\n",
            "Fetching https://non-existent-domain-12345.com via proxy http://190.61.88.147:8080\n",
            "Fetching http://httpbin.org/get?job=10 via proxy http://5.78.69.164:8080\n",
            "Error fetching http://httpbin.org/get?job=8: Cannot connect to host 194.135.18.239:5678 ssl:default [Connect call failed ('194.135.18.239', 5678)]\n",
            "Error fetching https://non-existent-domain-12345.com: \n",
            "Error fetching http://httpbin.org/get?job=10: \n",
            "\n",
            "--- Scraping Results ---\n",
            "   job_id                                    url  status_code  \\\n",
            "0       1           http://httpbin.org/get?job=1           -1   \n",
            "1       2           http://httpbin.org/get?job=2           -1   \n",
            "2       3          http://httpbin.org/status/404           -1   \n",
            "3       4           http://httpbin.org/get?job=4           -1   \n",
            "4       5             http://httpbin.org/delay/2           -1   \n",
            "5       6           http://httpbin.org/get?job=6           -1   \n",
            "6       7           http://httpbin.org/get?job=7           -1   \n",
            "7       8           http://httpbin.org/get?job=8           -1   \n",
            "8       9  https://non-existent-domain-12345.com           -1   \n",
            "9      10          http://httpbin.org/get?job=10           -1   \n",
            "\n",
            "                                       response_text  \n",
            "0                                                     \n",
            "1  Cannot connect to host 194.135.18.239:5678 ssl...  \n",
            "2                                                     \n",
            "3                                                     \n",
            "4  Cannot connect to host 194.135.18.239:5678 ssl...  \n",
            "5                                                     \n",
            "6                                                     \n",
            "7  Cannot connect to host 194.135.18.239:5678 ssl...  \n",
            "8                                                     \n",
            "9                                                     \n",
            "\n",
            "Saving results to table 'jobs_with_responses'...\n",
            "Database updated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect(DB_PATH)\n",
        "df_res = pd.read_sql_query(\"SELECT * from jobs_with_responses\", conn)\n",
        "df_res"
      ],
      "metadata": {
        "id": "hna1hnujL5y0",
        "outputId": "fddb88bd-6b31-464b-ea7f-b7eeca484861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   job_id                                    url  status_code  \\\n",
              "0       1             http://httpbin.org/delay/2           -1   \n",
              "1       2             http://httpbin.org/delay/2           -1   \n",
              "2       3             http://httpbin.org/delay/2           -1   \n",
              "3       4      https://api.ipify.org?format=json           -1   \n",
              "4       5      https://api.ipify.org?format=json           -1   \n",
              "5       6      https://api.ipify.org?format=json           -1   \n",
              "6       7      https://api.ipify.org?format=json           -1   \n",
              "7       8          http://httpbin.org/status/404           -1   \n",
              "8       9  https://non-existent-domain-12345.com           -1   \n",
              "\n",
              "                                       response_text  \n",
              "0                                                     \n",
              "1                                                     \n",
              "2                                                     \n",
              "3                                                     \n",
              "4                                                     \n",
              "5                                                     \n",
              "6                                                     \n",
              "7  Cannot connect to host 194.135.18.239:5678 ssl...  \n",
              "8                                                     "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b06fe157-7c89-48bc-82be-a227b6ef510c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>job_id</th>\n",
              "      <th>url</th>\n",
              "      <th>status_code</th>\n",
              "      <th>response_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>http://httpbin.org/delay/2</td>\n",
              "      <td>-1</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>http://httpbin.org/delay/2</td>\n",
              "      <td>-1</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>http://httpbin.org/delay/2</td>\n",
              "      <td>-1</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>https://api.ipify.org?format=json</td>\n",
              "      <td>-1</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>https://api.ipify.org?format=json</td>\n",
              "      <td>-1</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>https://api.ipify.org?format=json</td>\n",
              "      <td>-1</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>https://api.ipify.org?format=json</td>\n",
              "      <td>-1</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>http://httpbin.org/status/404</td>\n",
              "      <td>-1</td>\n",
              "      <td>Cannot connect to host 194.135.18.239:5678 ssl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>https://non-existent-domain-12345.com</td>\n",
              "      <td>-1</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b06fe157-7c89-48bc-82be-a227b6ef510c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b06fe157-7c89-48bc-82be-a227b6ef510c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b06fe157-7c89-48bc-82be-a227b6ef510c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0f55854b-ed8e-4b70-934c-47535aff8c46\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0f55854b-ed8e-4b70-934c-47535aff8c46')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0f55854b-ed8e-4b70-934c-47535aff8c46 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_26463272-234b-4da9-b2d7-c29672b9d067\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_res')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_26463272-234b-4da9-b2d7-c29672b9d067 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_res');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_res",
              "summary": "{\n  \"name\": \"df_res\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"job_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 9,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          8,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"https://api.ipify.org?format=json\",\n          \"https://non-existent-domain-12345.com\",\n          \"http://httpbin.org/delay/2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status_code\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": -1,\n        \"max\": -1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response_text\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Cannot connect to host 194.135.18.239:5678 ssl:default [Connect call failed ('194.135.18.239', 5678)]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import aiohttp\n",
        "from itertools import cycle\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# --- Configuration ---\n",
        "DB_PATH = 'jobs_database_advanced.db'\n",
        "PROXY_TABLE = 'proxies'\n",
        "JOBS_TABLE = 'jobs'\n",
        "RESULTS_TABLE = 'jobs_with_responses'\n",
        "\n",
        "# Global limit: At most 10 requests can be active across ALL domains.\n",
        "GLOBAL_MAX_CONCURRENT_REQUESTS = 10\n",
        "\n",
        "# Per-domain rules:\n",
        "# - 'httpbin.org' is limited to 2 concurrent requests.\n",
        "# - Any other domain will use the '__default__' limit of 3.\n",
        "DOMAIN_CONCURRENCY_RULES = {\n",
        "    'httpbin.org': 2,\n",
        "    '__default__': 3  # Default limit for any other domain\n",
        "}\n",
        "\n",
        "# --- 1. Database Setup (Helper function to create sample data) ---\n",
        "def setup_database():\n",
        "    \"\"\"Creates a sample database with proxies and jobs for demonstration.\"\"\"\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    c = conn.cursor()\n",
        "\n",
        "    # Proxies Table\n",
        "    c.execute(f'DROP TABLE IF EXISTS {PROXY_TABLE}')\n",
        "    c.execute(f'CREATE TABLE {PROXY_TABLE} (proxy_url TEXT PRIMARY KEY, active INTEGER)')\n",
        "    proxies = [\n",
        "        ('http://5.78.69.164:8080', 1), ('http://194.135.18.239:5678', 1),\n",
        "        ('http://1.1.1.1:8080', 0), ('http://190.61.88.147:8080', 1),\n",
        "    ]\n",
        "    c.executemany(f'INSERT INTO {PROXY_TABLE} VALUES (?, ?)', proxies)\n",
        "\n",
        "    # Jobs Table with URLs from different domains\n",
        "    c.execute(f'DROP TABLE IF EXISTS {JOBS_TABLE}')\n",
        "    c.execute(f'CREATE TABLE {JOBS_TABLE} (job_id INTEGER PRIMARY KEY, url TEXT NOT NULL)')\n",
        "    urls = [\n",
        "        (1, 'http://httpbin.org/delay/2'), (2, 'http://httpbin.org/delay/2'),\n",
        "        (3, 'http://httpbin.org/delay/2'), # These 3 will be slow due to domain limit\n",
        "        (4, 'https://api.ipify.org?format=json'), (5, 'https://api.ipify.org?format=json'),\n",
        "        (6, 'https://api.ipify.org?format=json'), (7, 'https://api.ipify.org?format=json'), # These 4 hit the default limit\n",
        "        (8, 'http://httpbin.org/status/404'), # Error URL\n",
        "        (9, 'https://non-existent-domain-12345.com'), # Connection error\n",
        "    ]\n",
        "    c.executemany(f'INSERT INTO {JOBS_TABLE} VALUES (?, ?)', urls)\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(\"Database setup complete.\")\n",
        "\n",
        "# --- 2. Core Asynchronous Logic ---\n",
        "\n",
        "async def fetch_url(session: aiohttp.ClientSession, url: str, proxy: str):\n",
        "    \"\"\"\n",
        "    Fetches a single URL using a given proxy.\n",
        "    Returns a dictionary with the URL, status, and response text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        timeout = aiohttp.ClientTimeout(total=30)\n",
        "        async with session.get(url, proxy=proxy, timeout=timeout) as response:\n",
        "            return {\n",
        "                \"url\": url,\n",
        "                \"status_code\": response.status,\n",
        "                \"response_text\": await response.text()\n",
        "            }\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {url}: {type(e).__name__}\")\n",
        "        return {\n",
        "            \"url\": url,\n",
        "            \"status_code\": -1,\n",
        "            \"response_text\": str(e)\n",
        "        }\n",
        "\n",
        "async def fetch_with_semaphores(url, proxy, session, global_sem, domain_sem):\n",
        "    \"\"\"\n",
        "    A wrapper that acquires both the global and domain-specific semaphores\n",
        "    before calling the fetch function.\n",
        "    \"\"\"\n",
        "    async with global_sem:\n",
        "        async with domain_sem:\n",
        "            # Announce which domain is being processed\n",
        "            domain = urlparse(url).netloc\n",
        "            print(f\"Processing {url} (Domain: {domain}) via proxy {proxy}\")\n",
        "            return await fetch_url(session, url, proxy)\n",
        "\n",
        "async def main():\n",
        "    \"\"\"Main function to orchestrate the entire process.\"\"\"\n",
        "    # a. Read data from the database\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    try:\n",
        "        proxies_df = pd.read_sql_query(f\"SELECT proxy_url FROM {PROXY_TABLE} WHERE active = 1\", conn)\n",
        "        jobs_df = pd.read_sql_query(f\"SELECT * FROM {JOBS_TABLE}\", conn)\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "    active_proxies = proxies_df['proxy_url'].tolist()\n",
        "    if not active_proxies:\n",
        "        print(\"No active proxies found. Exiting.\")\n",
        "        return\n",
        "    proxy_rotator = cycle(active_proxies)\n",
        "\n",
        "    # b. Initialize semaphores\n",
        "    global_semaphore = asyncio.Semaphore(GLOBAL_MAX_CONCURRENT_REQUESTS)\n",
        "    domain_semaphores = {}\n",
        "    default_limit = DOMAIN_CONCURRENCY_RULES['__default__']\n",
        "\n",
        "    # c. Create tasks with dual-semaphore logic\n",
        "    tasks = []\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        for _, row in jobs_df.iterrows():\n",
        "            url = row['url']\n",
        "            domain = urlparse(url).netloc\n",
        "\n",
        "            # Get or create the semaphore for this specific domain\n",
        "            if domain not in domain_semaphores:\n",
        "                limit = DOMAIN_CONCURRENCY_RULES.get(domain, default_limit)\n",
        "                print(f\"Creating semaphore for domain '{domain}' with limit {limit}.\")\n",
        "                domain_semaphores[domain] = asyncio.Semaphore(limit)\n",
        "\n",
        "            domain_semaphore = domain_semaphores[domain]\n",
        "\n",
        "            # Create a task that is governed by BOTH semaphores\n",
        "            task = asyncio.create_task(fetch_with_semaphores(\n",
        "                url, next(proxy_rotator), session, global_semaphore, domain_semaphore\n",
        "            ))\n",
        "            tasks.append(task)\n",
        "\n",
        "        # d. Run all tasks and gather results\n",
        "        print(f\"\\nStarting {len(tasks)} requests with global limit={GLOBAL_MAX_CONCURRENT_REQUESTS} and per-domain rules...\")\n",
        "        results = await asyncio.gather(*tasks)\n",
        "\n",
        "    # e. Process results and append to DataFrame\n",
        "    result_map = {res['url']: res for res in results}\n",
        "    jobs_df['status_code'] = jobs_df['url'].map(lambda u: result_map[u]['status_code'])\n",
        "    jobs_df['response_text'] = jobs_df['url'].map(lambda u: result_map[u]['response_text'][:500])\n",
        "\n",
        "    print(\"\\n--- Scraping Results ---\")\n",
        "    print(jobs_df)\n",
        "\n",
        "    # f. Update database with results\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    try:\n",
        "        print(f\"\\nSaving results to table '{RESULTS_TABLE}'...\")\n",
        "        jobs_df.to_sql(RESULTS_TABLE, conn, if_exists='replace', index=False)\n",
        "        print(\"Database updated successfully.\")\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    setup_database()\n",
        "    await main()"
      ],
      "metadata": {
        "id": "B1zP_NuaQtE3",
        "outputId": "59b740a5-d8fd-4358-9453-307a5f719d31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database setup complete.\n",
            "Creating semaphore for domain 'httpbin.org' with limit 2.\n",
            "Creating semaphore for domain 'api.ipify.org' with limit 3.\n",
            "Creating semaphore for domain 'non-existent-domain-12345.com' with limit 3.\n",
            "\n",
            "Starting 9 requests with global limit=10 and per-domain rules...\n",
            "Processing http://httpbin.org/delay/2 (Domain: httpbin.org) via proxy http://5.78.69.164:8080\n",
            "Processing http://httpbin.org/delay/2 (Domain: httpbin.org) via proxy http://194.135.18.239:5678\n",
            "Processing https://api.ipify.org?format=json (Domain: api.ipify.org) via proxy http://5.78.69.164:8080\n",
            "Processing https://api.ipify.org?format=json (Domain: api.ipify.org) via proxy http://194.135.18.239:5678\n",
            "Processing https://api.ipify.org?format=json (Domain: api.ipify.org) via proxy http://190.61.88.147:8080\n",
            "Processing https://non-existent-domain-12345.com (Domain: non-existent-domain-12345.com) via proxy http://190.61.88.147:8080\n",
            "Error fetching https://api.ipify.org?format=json: ClientProxyConnectionError\n",
            "Processing https://api.ipify.org?format=json (Domain: api.ipify.org) via proxy http://5.78.69.164:8080\n",
            "Error fetching http://httpbin.org/delay/2: ClientProxyConnectionError\n",
            "Processing http://httpbin.org/delay/2 (Domain: httpbin.org) via proxy http://190.61.88.147:8080\n",
            "Error fetching http://httpbin.org/delay/2: TimeoutError\n",
            "Error fetching https://api.ipify.org?format=json: TimeoutError\n",
            "Error fetching https://api.ipify.org?format=json: TimeoutError\n",
            "Error fetching https://non-existent-domain-12345.com: TimeoutError\n",
            "Error fetching http://httpbin.org/delay/2: TimeoutError\n",
            "Error fetching https://api.ipify.org?format=json: TimeoutError\n",
            "Processing http://httpbin.org/status/404 (Domain: httpbin.org) via proxy http://194.135.18.239:5678\n",
            "Error fetching http://httpbin.org/status/404: ClientProxyConnectionError\n",
            "\n",
            "--- Scraping Results ---\n",
            "   job_id                                    url  status_code  \\\n",
            "0       1             http://httpbin.org/delay/2           -1   \n",
            "1       2             http://httpbin.org/delay/2           -1   \n",
            "2       3             http://httpbin.org/delay/2           -1   \n",
            "3       4      https://api.ipify.org?format=json           -1   \n",
            "4       5      https://api.ipify.org?format=json           -1   \n",
            "5       6      https://api.ipify.org?format=json           -1   \n",
            "6       7      https://api.ipify.org?format=json           -1   \n",
            "7       8          http://httpbin.org/status/404           -1   \n",
            "8       9  https://non-existent-domain-12345.com           -1   \n",
            "\n",
            "                                       response_text  \n",
            "0                                                     \n",
            "1                                                     \n",
            "2                                                     \n",
            "3                                                     \n",
            "4                                                     \n",
            "5                                                     \n",
            "6                                                     \n",
            "7  Cannot connect to host 194.135.18.239:5678 ssl...  \n",
            "8                                                     \n",
            "\n",
            "Saving results to table 'jobs_with_responses'...\n",
            "Database updated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import http.server\n",
        "import threading\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "\n",
        "token = None\n",
        "\n",
        "def setup():\n",
        "    resp = requests.post('https://github.com/login/device/code', headers={\n",
        "            'accept': 'application/json',\n",
        "            'editor-version': 'Neovim/0.6.1',\n",
        "            'editor-plugin-version': 'copilot.vim/1.16.0',\n",
        "            'content-type': 'application/json',\n",
        "            'user-agent': 'GithubCopilot/1.155.0',\n",
        "            'accept-encoding': 'gzip,deflate,br'\n",
        "        }, data='{\"client_id\":\"Iv1.b507a08c87ecfe98\",\"scope\":\"read:user\"}')\n",
        "\n",
        "\n",
        "    # Parse the response json, isolating the device_code, user_code, and verification_uri\n",
        "    resp_json = resp.json()\n",
        "    device_code = resp_json.get('device_code')\n",
        "    user_code = resp_json.get('user_code')\n",
        "    verification_uri = resp_json.get('verification_uri')\n",
        "\n",
        "    # Print the user code and verification uri\n",
        "    print(f'Please visit {verification_uri} and enter code {user_code} to authenticate.')\n",
        "\n",
        "\n",
        "    while True:\n",
        "        time.sleep(5)\n",
        "        resp = requests.post('https://github.com/login/oauth/access_token', headers={\n",
        "            'accept': 'application/json',\n",
        "            'editor-version': 'Neovim/0.6.1',\n",
        "            'editor-plugin-version': 'copilot.vim/1.16.0',\n",
        "            'content-type': 'application/json',\n",
        "            'user-agent': 'GithubCopilot/1.155.0',\n",
        "            'accept-encoding': 'gzip,deflate,br'\n",
        "            }, data=f'{{\"client_id\":\"Iv1.b507a08c87ecfe98\",\"device_code\":\"{device_code}\",\"grant_type\":\"urn:ietf:params:oauth:grant-type:device_code\"}}')\n",
        "\n",
        "        # Parse the response json, isolating the access_token\n",
        "        resp_json = resp.json()\n",
        "        access_token = resp_json.get('access_token')\n",
        "\n",
        "        if access_token:\n",
        "            break\n",
        "\n",
        "    # Save the access token to a file\n",
        "    with open('.copilot_token', 'w') as f:\n",
        "        f.write(access_token)\n",
        "\n",
        "    print('Authentication success!')\n",
        "\n",
        "\n",
        "def get_token():\n",
        "    global token\n",
        "        # Check if the .copilot_token file exists\n",
        "    while True:\n",
        "        try:\n",
        "            with open('.copilot_token', 'r') as f:\n",
        "                access_token = f.read()\n",
        "                break\n",
        "        except FileNotFoundError:\n",
        "            setup()\n",
        "    # Get a session with the access token\n",
        "    resp = requests.get('https://api.github.com/copilot_internal/v2/token', headers={\n",
        "        'authorization': f'token {access_token}',\n",
        "        'editor-version': 'Neovim/0.6.1',\n",
        "        'editor-plugin-version': 'copilot.vim/1.16.0',\n",
        "        'user-agent': 'GithubCopilot/1.155.0'\n",
        "    })\n",
        "\n",
        "    # Parse the response json, isolating the token\n",
        "    resp_json = resp.json()\n",
        "    token = resp_json.get('token')\n",
        "\n",
        "\n",
        "def token_thread():\n",
        "    global token\n",
        "    while True:\n",
        "        get_token()\n",
        "        time.sleep(25 * 60)\n",
        "def chat_completion_stream(messages: list, model: str, temperature: float = 0.1):\n",
        "      \"\"\"Stream chat completion from Copilot\"\"\"\n",
        "      global token\n",
        "    # If the token is None, get a new one\n",
        "      if token is None or is_token_invalid(token):\n",
        "        get_token()\n",
        "      headers = {\n",
        "          'authorization': f'Bearer {token}',\n",
        "          'content-type': 'application/json',\n",
        "          'copilot-integration-id': 'vscode-chat',\n",
        "          'editor-plugin-version': 'copilot-chat/0.28.5',\n",
        "          'editor-version': 'vscode/1.101.2',\n",
        "          'openai-intent': 'conversation-other',\n",
        "          'user-agent': 'GitHubCopilotChat/0.28.5',\n",
        "          'x-github-api-version': '2025-05-01',\n",
        "          'x-initiator': 'user',\n",
        "          'x-interaction-type': 'conversation-other'\n",
        "      }\n",
        "\n",
        "      payload = {\n",
        "          \"messages\": messages,\n",
        "          \"model\": model,\n",
        "          \"temperature\": temperature,\n",
        "          \"top_p\": 1,\n",
        "          \"max_tokens\": 64000,\n",
        "          \"n\": 1,\n",
        "          \"stream\": True\n",
        "      }\n",
        "\n",
        "      try:\n",
        "          resp = requests.post('https://api.individual.githubcopilot.com/chat/completions',\n",
        "                              headers=headers, json=payload, stream=True)\n",
        "\n",
        "          if resp.status_code != 200:\n",
        "              yield f\"data: {json.dumps({'error': f'HTTP {resp.status_code}: {resp.text}'})}\\n\\n\"\n",
        "              return\n",
        "\n",
        "          for line in resp.iter_lines():\n",
        "              if line:\n",
        "                  line_str = line.decode('utf-8')\n",
        "                  if line_str.startswith('data: '):\n",
        "                      yield line_str + '\\n\\n'\n",
        "\n",
        "      except requests.exceptions.RequestException as e:\n",
        "          yield f\"data: {json.dumps({'error': f'Request error: {str(e)}'})}\\n\\n\"\n",
        "def copilot(prompt, language='python'):\n",
        "    global token\n",
        "    # If the token is None, get a new one\n",
        "    if token is None or is_token_invalid(token):\n",
        "        get_token()\n",
        "    header = {\n",
        "        'authorization': f'Bearer {token}',\n",
        "  \"ontent-type\": \"application/json\",\n",
        "  \"copilot-integration-id\": \"vscode-chat\",\n",
        "  \"editor-plugin-version\": \"copilot-chat/0.28.5\",\n",
        "  \"editor-version\": \"vscode/1.101.2\",\n",
        "  \"openai-intent\": \"conversation-other\",\n",
        "  \"user-agent\": \"GitHubCopilotChat/0.28.5\",\n",
        "  # \"vscode-machineid\": \"4547ee588f224680bd8fd598883975944a13b163e4b18f0f63de8950796578fe\",\n",
        "  # \"vscode-sessionid\": \"7a2c2033-6471-4682-bda7-d14e756a00781751890740497\",\n",
        "  \"x-github-api-version\": \"2025-05-01\",\n",
        "  \"x-initiator\": \"user\",\n",
        "  # \"x-interaction-id\": \"890a4322-84ee-42e9-96b8-b3f3ff44e529\",\n",
        "  \"x-interaction-type\": \"conversation-other\",\n",
        "  # \"x-onbehalf-extension-id\": \"saoudrizwan.claude-dev/3.18.3\",\n",
        "  # \"x-request-id\": \"31270fce-3828-4ce7-8e01-300521a9ca48\",\n",
        "  \"x-vscode-user-agent-library-version\": \"electron-fetch\",\n",
        "  \"sec-fetch-site\": \"none\",\n",
        "  \"sec-fetch-mode\": \"no-cors\",\n",
        "  \"sec-fetch-dest\": \"empty\",\n",
        "  \"accept-encoding\": \"gzip, deflate, br, zstd\",\n",
        "  \"priority\": \"u=4, i\"\n",
        "}\n",
        "    try:\n",
        "        resp = requests.post('https://api.individual.githubcopilot.com/chat/completions', headers=header,\n",
        "        json={\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Follow Microsoft content policies.\\nAvoid content that violates copyrights.\\nIf you are asked to generate content that is harmful, hateful, racist, sexist, lewd, or violent, only respond with \\\"Sorry, I can't assist with that.\\\"\\nKeep your answers short and impersonal.\\nUse Markdown formatting in your answers.\\nMake sure to include the programming language name at the start of the Markdown code blocks.\\nAvoid wrapping the whole response in triple backticks.\\nThe user works in an IDE called Visual Studio Code which has a concept for editors with open files, integrated unit test support, an output pane that shows the output of running the code as well as an integrated terminal.\\nThe active document is the source code the user is looking at right now.\\nYou can only give one reply for each conversation turn.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\" : \"assistant\",\n",
        "            \"content\" : prompt\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"<task>\\nhi\\n</task>\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"<environment_details>\\n# VSCode Visible Files\\n(No visible files)\\n\\n# VSCode Open Tabs\\nmain.py\\n../../../Users/madha/AppData/Roaming/Code/User/settings.json\\n\\n# Current Time\\n7/7/2025, 6:30:50 PM (Asia/Calcutta, UTC+5.5:00)\\n\\n# Current Working Directory (c:/my_space/projects/vscode_copilot) Files\\n.copilot_token\\nmain.py\\n\\n# Context Window Usage\\n0 / 128K tokens used (0%)\\n\\n# Current Mode\\nPLAN MODE\\nIn this mode you should focus on information gathering, asking questions, and architecting a solution. Once you have a plan, use the plan_mode_respond tool to engage in a conversational back and forth with the user. Do not use the plan_mode_respond tool until you've gathered all the information you need e.g. with read_file or ask_followup_question.\\n(Remember: If it seems the user wants you to use tools only available in Act Mode, you should ask the user to \\\"toggle to Act mode\\\" (use those words) - they will have to manually do this themselves with the Plan/Act toggle button below. You do not have the ability to switch to Act Mode yourself, and must wait for the user to do it themselves once they are satisfied with the plan. You also cannot present an option to toggle to Act mode, as this will be something you need to direct the user to do manually themselves.)\\n</environment_details>\"\n",
        "        }\n",
        "    ],\n",
        "    \"model\": \"gemini-2.5-pro\",\n",
        "    \"temperature\": 0.1,\n",
        "    \"top_p\": 1,\n",
        "    \"max_tokens\": 64000,\n",
        "    \"n\": 1,\n",
        "    \"stream\": True\n",
        "}\n",
        "                             )\n",
        "        print(resp.status_code)\n",
        "        print(resp.text)\n",
        "\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        return ''\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    # Parse the response text, splitting it by newlines\n",
        "    resp_text = resp.text.split('\\n')\n",
        "    for line in resp_text:\n",
        "        # If the line contains a completion, print it\n",
        "        if line.startswith('data: {'):\n",
        "            # Parse the completion from the line as json\n",
        "            json_completion = json.loads(line[6:])\n",
        "            completion = json_completion.get('choices')[0].get('text')\n",
        "            if completion:\n",
        "                result += completion\n",
        "            else:\n",
        "                result += '\\n'\n",
        "\n",
        "    return result\n",
        "\n",
        "# Check if the token is invalid through the exp field\n",
        "def is_token_invalid(token):\n",
        "    if token is None or 'exp' not in token or extract_exp_value(token) <= time.time():\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def extract_exp_value(token):\n",
        "    pairs = token.split(';')\n",
        "    for pair in pairs:\n",
        "        key, value = pair.split('=')\n",
        "        if key.strip() == 'exp':\n",
        "            return int(value.strip())\n",
        "    return None\n",
        "\n",
        "# class HTTPRequestHandler(http.server.BaseHTTPRequestHandler):\n",
        "#     def do_POST(self):\n",
        "#         # Get the request body\n",
        "#         content_length = int(self.headers['Content-Length'])\n",
        "#         body = self.rfile.read(content_length)\n",
        "\n",
        "#         # Parse the request body as json\n",
        "#         body_json = json.loads(body)\n",
        "\n",
        "#         # Get the prompt from the request body\n",
        "#         prompt = body_json.get('prompt')\n",
        "#         language = body_json.get('language', 'python')\n",
        "\n",
        "#         # Get the completion from the copilot function\n",
        "#         completion = copilot(prompt, language)\n",
        "\n",
        "#         # Send the completion as the response\n",
        "#         self.send_response(200)\n",
        "#         self.send_header('Content-type', 'text/plain')\n",
        "#         self.end_headers()\n",
        "#         self.wfile.write(completion.encode())\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Every 25 minutes, get a new token\n",
        "    threading.Thread(target=token_thread, daemon=True).start()\n",
        "    prompt = [\"generate me a addition function in python\"]\n",
        "    language = \"python\"\n",
        "    # completion = copilot(prompt, language)\n",
        "    completion_generator = chat_completion_stream(prompt,\"gemini-2.5-pro\")\n",
        "    for chunk in completion_generator:\n",
        "      print(chunk, end='')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "Q-z919iR1elC",
        "outputId": "2a34f5ed-bc95-476f-c28a-da3a90f6af74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please visit https://github.com/login/device and enter code 97D7-AFAF to authenticate.\n",
            "Please visit https://github.com/login/device and enter code 0D0C-5F00 to authenticate.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-1266996346.py\u001b[0m in \u001b[0;36mget_token\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.copilot_token'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0maccess_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.copilot_token'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-1266996346.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2-1266996346.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;31m# completion = copilot(prompt, language)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0mcompletion_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat_completion_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"gemini-2.5-pro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompletion_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2-1266996346.py\u001b[0m in \u001b[0;36mchat_completion_stream\u001b[0;34m(messages, model, temperature)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# If the token is None, get a new one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_token_invalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mget_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m       headers = {\n\u001b[1;32m     91\u001b[0m           \u001b[0;34m'authorization'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf'Bearer {token}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2-1266996346.py\u001b[0m in \u001b[0;36mget_token\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Get a session with the access token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     resp = requests.get('https://api.github.com/copilot_internal/v2/token', headers={\n",
            "\u001b[0;32m/tmp/ipython-input-2-1266996346.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         resp = requests.post('https://github.com/login/oauth/access_token', headers={\n\u001b[1;32m     34\u001b[0m             \u001b[0;34m'accept'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'application/json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "print(token)\n",
        "token = json.loads(token)\n",
        "headers = {'authorization': f'Bearer {token[\"tid\"]}'}\n",
        "print(headers)"
      ],
      "metadata": {
        "id": "bBdxfwmzvvmG",
        "outputId": "9d8e59be-80ce-4fbe-81d3-4104dddaa095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tid=0b97ef2bd0dcc75c83d5ca765e9d6a4c;exp=1751891450;sku=free_educational_quota;proxy-ep=proxy.individual.githubcopilot.com;st=dotcom;chat=1;cit=1;malfil=1;editor_preview_features=1;mcp=1;ccr=1;rt=1;8kp=1;ip=34.23.59.211;asn=AS396982:1df83fc4b78787daa76ade7b527ded8e9481e6a9bcf4f2ee3c6d9c6778155e40\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Expecting value: line 1 column 1 (char 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13-71085708.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'authorization'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf'Bearer {token[\"tid\"]}'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def copilot(prompt, language='python'):\n",
        "    global token\n",
        "    # If the token is None or invalid, get a new one\n",
        "    # Note: The is_token_invalid function also needs a small fix (see below)\n",
        "    if is_token_invalid(token):\n",
        "        get_token()\n",
        "\n",
        "    # --- FIX #1: Use the correct token string from the JSON object ---\n",
        "    # The header needs the actual token, not the whole python dictionary.\n",
        "    # headers = {'authorization': f'Bearer {token[\"tid\"]}'}\n",
        "    headers = {'authorization': f'Bearer {token}'}\n",
        "\n",
        "    json_payload = {\n",
        "        'prompt': prompt,\n",
        "        'suffix': '',\n",
        "        'max_tokens': 1000, # 10000 is very large, using a more standard value\n",
        "        'temperature': 0,\n",
        "        'top_p': 1,\n",
        "        'n': 1,\n",
        "        'stop': ['\\n'],\n",
        "        'nwo': 'github/copilot.vim',\n",
        "        'stream': True,\n",
        "        # 'extra': {\n",
        "        #     'language': language\n",
        "        # }\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # --- FIX #2: Process the response as a stream ---\n",
        "        # We add stream=True to the request call itself and iterate over the response.\n",
        "        with requests.post('https://copilot-proxy.githubusercontent.com/v1/engines/copilot-codex/completions',\n",
        "                           headers=headers, json=json_payload, stream=True) as resp:\n",
        "            print(resp)\n",
        "            print(f\"Status Code: {resp.status_code}\")\n",
        "            if resp.status_code != 200:\n",
        "                print(f\"Error: Received status code {resp.status_code}\")\n",
        "                print(resp.text)\n",
        "                return \"\"\n",
        "\n",
        "            result = ''\n",
        "            # We iterate over the response line by line as it comes in\n",
        "            for line in resp.iter_lines():\n",
        "                if not line:\n",
        "                    continue\n",
        "\n",
        "                # The line is a bytes object, so we decode it\n",
        "                line_str = line.decode('utf-8')\n",
        "\n",
        "                # The stream ends with this special message\n",
        "                if line_str == 'data: [DONE]':\n",
        "                    break\n",
        "\n",
        "                # We're looking for the data payload\n",
        "                if line_str.startswith('data: {'):\n",
        "                    try:\n",
        "                        # Parse the completion from the line as json\n",
        "                        json_completion = json.loads(line_str[6:])\n",
        "\n",
        "                        # --- FIX #3: Use the correct JSON key 'p' instead of 'text' ---\n",
        "                        completion = json_completion.get('choices')[0].get('p') # Changed .get('text') to .get('p')\n",
        "\n",
        "                        if completion:\n",
        "                            result += completion\n",
        "                    except (json.JSONDecodeError, IndexError, KeyError) as e:\n",
        "                        print(f\"Skipping malformed line: {line_str}, Error: {e}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"A connection error occurred: {e}\")\n",
        "        return ''\n",
        "\n",
        "def main():\n",
        "    # Every 25 minutes, get a new token\n",
        "    threading.Thread(target=token_thread, daemon=True).start()\n",
        "    prompt = \"what model are you\"\n",
        "    language = \"python\"\n",
        "    completion = copilot(prompt, language)\n",
        "    print(completion)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "wl0famk92WA3",
        "outputId": "488797ff-0962-4154-e197-a5bf62871fca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n",
            "Status Code: 200\n",
            "aaaaaaaaaaaaaaa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fgoB0T_P5fQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6AuDXkmV5fDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bjd71-CCw6E1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}