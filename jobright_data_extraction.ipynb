{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chikilivighneshshastry/colab_files/blob/main/jobright_data_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# extract fingle page info from jonright\n"
      ],
      "metadata": {
        "id": "U5Ooor4StP2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import aiohttp\n",
        "import asyncio\n",
        "\n",
        "async def extract_job_data(url):\n",
        "  async with aiohttp.ClientSession() as session:\n",
        "    response = await session.get(url)\n",
        "    html = await response.text()\n",
        "    print(html)\n",
        "    return response\n",
        "\n",
        "url = 'https://jobright.ai/jobs/info/685a54b5be2d7e56476268d'\n",
        "# url = 'https://jobright.ai/jobs/info'\n",
        "response = await extract_job_data(url)"
      ],
      "metadata": {
        "id": "e1QKrxJOkCEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.status\n",
        "html_data = await response.text()\n",
        "print(html_data)"
      ],
      "metadata": {
        "id": "3k8MLjR-kFBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: parse html_data using bs4 and get with text in a id\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "soup = BeautifulSoup(html_data, 'html.parser')\n",
        "\n",
        "# Assuming the text you want is within an element with a specific ID,\n",
        "# replace 'your_element_id' with the actual ID of the element.\n",
        "element_with_id = soup.find(id='__NEXT_DATA__')\n",
        "\n",
        "if element_with_id:\n",
        "  detailed_json_data = element_with_id.get_text()\n",
        "  print(detailed_json_data)\n",
        "else:\n",
        "  print(\"Element with the specified ID not found.\")\n",
        "\n",
        "data = json.loads(detailed_json_data)\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "URqMe0gEkIGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.keys()\n",
        "print(data['props'].keys())\n",
        "print(data['page'])\n",
        "print(data['query'])\n",
        "print(data['buildId'])\n",
        "print(data['isFallback'])\n",
        "print(data['gssp'])\n",
        "print(data['scriptLoader'])"
      ],
      "metadata": {
        "id": "ol3dES5el8uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['props']['pageProps']['baseSalary'])\n",
        "print(data['props']['pageProps']['jobLocation'])\n",
        "print(data['props']['pageProps']['logined'])\n",
        "print(data['props']['pageProps']['jobHashedId'])\n",
        "print(data['props']['pageProps']['_sentryTraceData'])\n",
        "print(data['props']['pageProps']['_sentryBaggage'])"
      ],
      "metadata": {
        "id": "7G4nb_jAm9P1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['props']['pageProps']['dataSource']"
      ],
      "metadata": {
        "id": "eKJpjkecmnss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import random\n",
        "\n",
        "# --- Configuration ---\n",
        "TARGET_SITE_DOMAIN = \"example.com\" # To keep crawling within the site\n",
        "INITIAL_SEED_URL = f\"http://{TARGET_SITE_DOMAIN}\"\n",
        "\n",
        "PROXY_LIST = [\n",
        "    \"http://proxy1.com:8080\",\n",
        "    \"http://user:pass@proxy2.com:3128\",\n",
        "    # ... more proxies\n",
        "]\n",
        "\n",
        "# --- Database (Conceptual - replace with actual DB interaction) ---\n",
        "# In a real scenario, use libraries like psycopg2 (PostgreSQL), mysql.connector, sqlite3, or an ORM like SQLAlchemy\n",
        "DATABASE_URLS_SEEN = set() # Simple in-memory set for this example; use a real DB!\n",
        "\n",
        "async def db_url_exists(url):\n",
        "    # Simulate DB check\n",
        "    return url in DATABASE_URLS_SEEN\n",
        "\n",
        "async def db_add_url(url):\n",
        "    # Simulate DB add\n",
        "    DATABASE_URLS_SEEN.add(url)\n",
        "    print(f\"[DB] Added: {url}\")\n",
        "\n",
        "# --- Crawler Components ---\n",
        "url_frontier = asyncio.Queue()\n",
        "processed_urls_count = 0\n",
        "MAX_URLS_TO_CRAWL = 100 # Example limit\n",
        "\n",
        "async def fetch(session, url, proxy):\n",
        "    try:\n",
        "        print(f\"[FETCHING] {url} via proxy {proxy if proxy else 'DIRECT'}\")\n",
        "        async with session.get(url, proxy=proxy, timeout=10, ssl=False) as response: # Added ssl=False for potential local SSL issues\n",
        "            if response.status == 200:\n",
        "                return await response.text()\n",
        "            else:\n",
        "                print(f\"[ERROR] HTTP {response.status} for {url}\")\n",
        "                return None\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to fetch {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_proxy():\n",
        "    if PROXY_LIST:\n",
        "        return random.choice(PROXY_LIST)\n",
        "    return None\n",
        "\n",
        "def parse_and_extract_links(html_content, base_url):\n",
        "    links = set()\n",
        "    if not html_content:\n",
        "        return links\n",
        "    soup = BeautifulSoup(html_content, 'lxml') # 'html.parser' is a built-in alternative\n",
        "    for a_tag in soup.find_all('a', href=True):\n",
        "        href = a_tag['href']\n",
        "        # Join relative URLs with the base URL\n",
        "        full_url = urljoin(base_url, href)\n",
        "        # Basic clean-up (remove fragment, normalize)\n",
        "        parsed_url = urlparse(full_url)\n",
        "        normalized_url = parsed_url._replace(fragment=\"\").geturl()\n",
        "\n",
        "        # Filter: Only crawl URLs from the target domain\n",
        "        if urlparse(normalized_url).netloc == TARGET_SITE_DOMAIN:\n",
        "            links.add(normalized_url)\n",
        "    return links\n",
        "\n",
        "async def worker(name, session):\n",
        "    global processed_urls_count\n",
        "    while True:\n",
        "        try:\n",
        "            current_url = await url_frontier.get()\n",
        "            print(f\"[{name}] Processing: {current_url}\")\n",
        "\n",
        "            if await db_url_exists(current_url):\n",
        "                print(f\"[{name}] Already processed/in DB: {current_url}\")\n",
        "                url_frontier.task_done()\n",
        "                continue\n",
        "\n",
        "            await db_add_url(current_url) # Add to DB before fetching (or mark as being processed)\n",
        "\n",
        "            # TODO: Implement robots.txt check here\n",
        "\n",
        "            proxy = get_proxy()\n",
        "            html = await fetch(session, current_url, proxy)\n",
        "\n",
        "            if html:\n",
        "                new_links = parse_and_extract_links(html, current_url)\n",
        "                for link in new_links:\n",
        "                    if not await db_url_exists(link) and processed_urls_count < MAX_URLS_TO_CRAWL :\n",
        "                        # Check DB again before adding to frontier to handle race conditions if multiple workers find same link\n",
        "                        if link not in DATABASE_URLS_SEEN: # Simplified check; real DB would handle uniqueness\n",
        "                            await url_frontier.put(link)\n",
        "                            print(f\"[{name}] Queued new link: {link}\")\n",
        "\n",
        "\n",
        "                processed_urls_count += 1\n",
        "                if processed_urls_count >= MAX_URLS_TO_CRAWL:\n",
        "                    print(f\"[{name}] Reached max URL limit. Draining queue...\")\n",
        "                    # Allow other tasks to finish current work, then stop adding new ones.\n",
        "                    # Or more abruptly, cancel other tasks.\n",
        "\n",
        "            url_frontier.task_done()\n",
        "\n",
        "            if processed_urls_count >= MAX_URLS_TO_CRAWL and url_frontier.empty():\n",
        "                break # Exit worker if limit reached and queue is empty\n",
        "\n",
        "            await asyncio.sleep(1) # Be respectful: add a small delay\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[{name}] Error in worker: {e}\")\n",
        "            url_frontier.task_done() # Ensure task_done is called even on error\n",
        "            continue # Continue to next URL\n",
        "\n",
        "async def main():\n",
        "    await url_frontier.put(INITIAL_SEED_URL)\n",
        "    await db_add_url(INITIAL_SEED_URL) # Add seed to DB initially\n",
        "\n",
        "    # You might want a ClientSession per proxy type or a more sophisticated setup\n",
        "    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=False)) as session: # ssl=False for local dev; use proper SSL context in prod\n",
        "        # Create worker tasks\n",
        "        num_workers = 5 # Number of concurrent crawlers\n",
        "        tasks = []\n",
        "        for i in range(num_workers):\n",
        "            task = asyncio.create_task(worker(f\"Worker-{i+1}\", session))\n",
        "            tasks.append(task)\n",
        "\n",
        "        # Wait for the queue to be processed or limit to be reached\n",
        "        await url_frontier.join() # Waits until all items in queue are gotten and processed\n",
        "\n",
        "        # If max URLs reached, there might still be items in the queue\n",
        "        # or workers might be processing. We need a way to signal them to stop gracefully.\n",
        "        # For simplicity here, we cancel tasks if max_urls is hit and queue is effectively drained by workers.\n",
        "        if processed_urls_count >= MAX_URLS_TO_CRAWL:\n",
        "            print(\"Max URL limit reached. Cancelling worker tasks...\")\n",
        "\n",
        "        for task in tasks:\n",
        "            task.cancel() # Cancel all worker tasks\n",
        "\n",
        "        await asyncio.gather(*tasks, return_exceptions=True) # Wait for tasks to be cancelled\n",
        "\n",
        "    print(\"Crawling finished.\")\n",
        "    print(f\"Total unique URLs seen (from in-memory set): {len(DATABASE_URLS_SEEN)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "HoLY9OQf0Dlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crawl4ai.deep_crawling import BFSDeepCrawlStrategy\n",
        "\n",
        "strategy = BFSDeepCrawlStrategy(\n",
        "    max_depth=2,               # Crawl initial page + 2 levels deep\n",
        "    include_external=False,    # Stay within the same domain\n",
        "    max_pages=50,              # Maximum number of pages to crawl (optional)\n",
        "    score_threshold=0.3,       # Minimum score for URLs to be crawled (optional)\n",
        ")"
      ],
      "metadata": {
        "id": "UyXy8aFi3yMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install crawl4ai"
      ],
      "metadata": {
        "id": "_KTSp1Yu8TSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!crawl4ai-setup"
      ],
      "metadata": {
        "id": "nC2r8NWe8xB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n",
        "from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy\n",
        "from crawl4ai.deep_crawling import BestFirstCrawlingStrategy\n",
        "from crawl4ai.deep_crawling import BFSDeepCrawlStrategy\n",
        "from crawl4ai.deep_crawling.filters import (\n",
        "    FilterChain,\n",
        "    DomainFilter,\n",
        "    URLPatternFilter,\n",
        "    ContentTypeFilter\n",
        ")\n",
        "from crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\n",
        "\n",
        "async def run_advanced_crawler():\n",
        "    # Create a sophisticated filter chain\n",
        "    filter_chain = FilterChain([\n",
        "        # Domain boundaries\n",
        "        DomainFilter(\n",
        "            allowed_domains=[\"jobright.ai\"]\n",
        "            # blocked_domains=[\"old.docs.example.com\"]\n",
        "        ),\n",
        "\n",
        "        # URL patterns to include\n",
        "        # URLPatternFilter(patterns=[\"*guide*\", \"*tutorial*\", \"*blog*\"]),\n",
        "\n",
        "        # Content type filtering\n",
        "        ContentTypeFilter(allowed_types=[\"text/html\"])\n",
        "    ])\n",
        "\n",
        "\n",
        "    # Set up the configuration\n",
        "    config = CrawlerRunConfig(\n",
        "        deep_crawl_strategy =BFSDeepCrawlStrategy()\n",
        "\n",
        "\n",
        "        deep_crawl_strategy=BestFirstCrawlingStrategy(\n",
        "            max_depth=2,\n",
        "            include_external=False,\n",
        "            filter_chain=filter_chain\n",
        "        ),\n",
        "        scraping_strategy=LXMLWebScrapingStrategy(),\n",
        "        stream=True,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Execute the crawl\n",
        "    results = []\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        async for result in await crawler.arun(\"https://jobright.ai/jobs/info\", config=config):\n",
        "            results.append(result)\n",
        "            score = result.metadata.get(\"score\", 0)\n",
        "            depth = result.metadata.get(\"depth\", 0)\n",
        "            print(f\"Depth: {depth} | Score: {score:.2f} | {result.url}\")\n",
        "\n",
        "    # Analyze the results\n",
        "    print(f\"Crawled {len(results)} high-value pages\")\n",
        "    print(f\"Average score: {sum(r.metadata.get('score', 0) for r in results) / len(results):.2f}\")\n",
        "\n",
        "    # Group by depth\n",
        "    depth_counts = {}\n",
        "    for result in results:\n",
        "        depth = result.metadata.get(\"depth\", 0)\n",
        "        depth_counts[depth] = depth_counts.get(depth, 0) + 1\n",
        "\n",
        "    print(\"Pages crawled by depth:\")\n",
        "    for depth, count in sorted(depth_counts.items()):\n",
        "        print(f\"  Depth {depth}: {count} pages\")\n",
        "    return results\n",
        "if __name__ == \"__main__\":\n",
        "    results = await run_advanced_crawler()\n"
      ],
      "metadata": {
        "id": "aWeCA2m97xHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_url =''\n",
        "end_word_categorys = []\n",
        "parmeters = []\n",
        "result_urls = []\n",
        "for name in end_categorys:\n",
        "  for para in parmeters:\n",
        "    url = prepare_url(base_url,name,para)\n",
        "    result_urls = get_all_jobpost_urls(url)\n",
        "    save_to_db(result_urls)\n",
        "\n"
      ],
      "metadata": {
        "id": "PD_2GtYIB-dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scrapy"
      ],
      "metadata": {
        "id": "0wpppVN9cPA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "from scrapy.spiders import CrawlSpider, Rule\n",
        "from scrapy.linkextractors import LinkExtractor\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from scrapy.utils.project import get_project_settings # Useful for default settings\n",
        "\n",
        "# This is often needed if running in Jupyter/IPython to avoid \"ReactorNotRestartable\"\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- Your Spider Definition ---\n",
        "class AllLinksSpider(CrawlSpider):\n",
        "    name = 'all_links_scraper_cell' # Changed name slightly to avoid clashes if you have the other one\n",
        "    allowed_domains = ['jobright.ai']\n",
        "    # start_urls = ['https://jobright.ai/jobs/backenddeveloper/']\n",
        "    start_urls = ['https://jobright.ai/jobs/backenddeveloper'] # More general starting point\n",
        "    # Counter for processed URLs or items\n",
        "    processed_url_count = 0\n",
        "    MAX_URLS_TO_PROCESS = 100\n",
        "    # Custom settings for this spider if needed (can also be passed to CrawlerProcess)\n",
        "    custom_settings = {\n",
        "        'LOG_LEVEL': 'INFO', # 'DEBUG' for more verbosity\n",
        "        'DOWNLOAD_DELAY': 0.25,  # Be respectful\n",
        "        'CONCURRENT_REQUESTS_PER_DOMAIN': 8,\n",
        "        # 'DEPTH_LIMIT': 2 # Uncomment to limit crawl depth\n",
        "    }\n",
        "\n",
        "    rules = (\n",
        "        Rule(\n",
        "            LinkExtractor(\n",
        "                allow_domains=['jobright.ai'],\n",
        "                deny=(\n",
        "                    r'/login', r'/register', r'/password', # Example patterns to avoid\n",
        "                    r'mailto:', r'tel:', # Avoid mail and tel links\n",
        "                )\n",
        "            ),\n",
        "            callback='parse_page_links',\n",
        "            follow=True\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AllLinksSpider, self).__init__(*args, **kwargs)\n",
        "        # Get the list passed from the CrawlerProcess or default to an empty list\n",
        "        self.collected_links_list = kwargs.get('output_list', [])\n",
        "\n",
        "    def parse_page_links(self, response):\n",
        "        self.logger.info(f\"Processing page: {response.url}\")\n",
        "        links_on_this_page = response.css('a::attr(href)').getall()\n",
        "\n",
        "        for link_href in links_on_this_page:\n",
        "            absolute_link = response.urljoin(link_href)\n",
        "            # Check if it's within allowed domains again, just to be safe if LinkExtractor somehow missed\n",
        "            if self.allowed_domains and any(domain in scrapy.utils.url.get_domain(absolute_link) for domain in self.allowed_domains):\n",
        "                link_data = {\n",
        "                    'source_page_url': response.url,\n",
        "                    'extracted_link': absolute_link\n",
        "                }\n",
        "                # Append to the list provided during initialization\n",
        "                self.collected_links_list.append(link_data)\n",
        "                # Still yield if you want to use Scrapy's feed exporters or other pipelines\n",
        "                yield link_data"
      ],
      "metadata": {
        "id": "JC0b2n4ycLDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This list will be populated by the spider\n",
        "scraped_links_data = []\n",
        "\n",
        "# --- Configure and Run the Crawler ---\n",
        "# Get project settings if you have a settings.py, otherwise, it provides defaults\n",
        "settings = get_project_settings()\n",
        "\n",
        "# Override or add settings\n",
        "settings.set('USER_AGENT', 'MyCustomBot/1.0 (+http://mywebsite.com/botinfo)')\n",
        "# If you want to output to a file as well using Scrapy's feed exporters:\n",
        "# settings.set('FEEDS', {\n",
        "#     'output_links.json': {'format': 'json', 'overwrite': True},\n",
        "# })\n",
        "\n",
        "# Create a CrawlerProcess\n",
        "# The 'settings' argument can be a Settings object or a dictionary\n",
        "process = CrawlerProcess(settings=settings)\n",
        "\n",
        "# Pass the list to the spider instance when scheduling it\n",
        "# The spider's __init__ will pick up 'output_list' from kwargs\n",
        "process.crawl(AllLinksSpider, output_list=scraped_links_data)\n",
        "\n",
        "# The script will block here until all crawling is finished\n",
        "print(\"Starting Scrapy process...\")\n",
        "process.start()\n",
        "print(\"Scrapy process finished.\")\n",
        "\n",
        "# --- Now print the collected links ---\n",
        "print(f\"\\n--- Collected {len(scraped_links_data)} link entries: ---\")\n",
        "unique_extracted_links = set()\n",
        "for item in scraped_links_data:\n",
        "    print(f\"From: {item['source_page_url']} -> Found: {item['extracted_link']}\")\n",
        "    unique_extracted_links.add(item['extracted_link'])\n",
        "\n",
        "print(f\"\\n--- {len(unique_extracted_links)} Unique Extracted Links: ---\")\n",
        "for link in sorted(list(unique_extracted_links)): # Print sorted unique links\n",
        "    print(link)\n",
        "\n",
        "# If you want just a flat list of the unique extracted URLs:\n",
        "final_unique_links_list = sorted(list(unique_extracted_links))\n",
        "# print(\"\\nFinal flat list of unique links:\")\n",
        "# print(final_unique_links_list)"
      ],
      "metadata": {
        "id": "p2OIReYAgzK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "id": "kOiW1wfMl8M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply() # <--- ADD THIS LINE AT THE TOP\n",
        "\n",
        "import scrapy\n",
        "from scrapy.spiders import CrawlSpider, Rule\n",
        "from scrapy.linkextractors import LinkExtractor\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from scrapy.utils.project import get_project_settings\n",
        "\n",
        "# --- Your Spider Definition (AllLinksSpider or similar) ---\n",
        "class AllLinksSpider(CrawlSpider):\n",
        "    name = 'all_links_scraper_cell'\n",
        "    allowed_domains = ['jobright.ai']\n",
        "    start_urls = ['https://jobright.ai/jobs/']\n",
        "\n",
        "    custom_settings = {\n",
        "        'LOG_LEVEL': 'INFO',\n",
        "        'DOWNLOAD_DELAY': 0.25,\n",
        "        'CONCURRENT_REQUESTS_PER_DOMAIN': 8,\n",
        "        'CLOSESPIDER_ITEMCOUNT': 20 # Example limit: stop after 20 items\n",
        "    }\n",
        "\n",
        "    rules = (\n",
        "        Rule(\n",
        "            LinkExtractor(\n",
        "                allow_domains=['jobright.ai'],\n",
        "                deny=(r'/login', r'/register', r'/password', r'mailto:', r'tel:')\n",
        "            ),\n",
        "            callback='parse_page_links',\n",
        "            follow=True\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AllLinksSpider, self).__init__(*args, **kwargs)\n",
        "        self.collected_links_list = kwargs.get('output_list', [])\n",
        "\n",
        "    def parse_page_links(self, response):\n",
        "        # self.logger.info(f\"Processing page: {response.url}\") # Keep if needed\n",
        "        links_on_this_page = response.css('a::attr(href)').getall()\n",
        "\n",
        "        for link_href in links_on_this_page:\n",
        "            absolute_link = response.urljoin(link_href)\n",
        "            if self.allowed_domains and any(domain in scrapy.utils.url.get_domain(absolute_link) for domain in self.allowed_domains):\n",
        "                link_data = {\n",
        "                    'source_page_url': response.url,\n",
        "                    'extracted_link': absolute_link\n",
        "                }\n",
        "                self.collected_links_list.append(link_data)\n",
        "                yield link_data # Yielding is good for CLOSESPIDER_ITEMCOUNT\n",
        "\n",
        "# --- Script to Run the Spider ---\n",
        "scraped_links_data = []\n",
        "settings = get_project_settings()\n",
        "settings.set('USER_AGENT', 'MyCustomBot/1.0 (+http://mywebsite.com/botinfo)')\n",
        "\n",
        "process = CrawlerProcess(settings=settings)\n",
        "process.crawl(AllLinksSpider, output_list=scraped_links_data)\n",
        "\n",
        "print(\"Starting Scrapy process...\")\n",
        "process.start() # This should now work multiple times in the same session\n",
        "print(\"Scrapy process finished.\")\n",
        "\n",
        "print(f\"\\n--- Collected {len(scraped_links_data)} link entries (first 10 shown if many): ---\")\n",
        "for i, item in enumerate(scraped_links_data):\n",
        "    if i < 10: # Print only the first 10 for brevity\n",
        "        print(f\"From: {item['source_page_url']} -> Found: {item['extracted_link']}\")\n",
        "    elif i == 10:\n",
        "        print(\"... and more.\")\n",
        "        break\n",
        "\n",
        "unique_extracted_links = set(item['extracted_link'] for item in scraped_links_data)\n",
        "print(f\"\\n--- {len(unique_extracted_links)} Unique Extracted Links (first 10 shown if many): ---\")\n",
        "for i, link in enumerate(sorted(list(unique_extracted_links))):\n",
        "    if i < 10:\n",
        "        print(link)\n",
        "    elif i == 10:\n",
        "        print(\"... and more.\")\n",
        "        break"
      ],
      "metadata": {
        "id": "HEZ4bFh2l4jD",
        "outputId": "7378fa31-92da-42fe-a893-9cc020505f50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:scrapy.utils.log:Scrapy 2.13.2 started (bot: scrapybot)\n",
            "2025-06-24 15:39:24 [scrapy.utils.log] INFO: Scrapy 2.13.2 started (bot: scrapybot)\n",
            "INFO:scrapy.utils.log:Versions:\n",
            "{'lxml': '5.4.0',\n",
            " 'libxml2': '2.13.8',\n",
            " 'cssselect': '1.3.0',\n",
            " 'parsel': '1.10.0',\n",
            " 'w3lib': '2.3.1',\n",
            " 'Twisted': '25.5.0',\n",
            " 'Python': '3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]',\n",
            " 'pyOpenSSL': '25.1.0 (OpenSSL 3.3.2 3 Sep 2024)',\n",
            " 'cryptography': '43.0.3',\n",
            " 'Platform': 'Linux-6.1.123+-x86_64-with-glibc2.35'}\n",
            "2025-06-24 15:39:24 [scrapy.utils.log] INFO: Versions:\n",
            "{'lxml': '5.4.0',\n",
            " 'libxml2': '2.13.8',\n",
            " 'cssselect': '1.3.0',\n",
            " 'parsel': '1.10.0',\n",
            " 'w3lib': '2.3.1',\n",
            " 'Twisted': '25.5.0',\n",
            " 'Python': '3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]',\n",
            " 'pyOpenSSL': '25.1.0 (OpenSSL 3.3.2 3 Sep 2024)',\n",
            " 'cryptography': '43.0.3',\n",
            " 'Platform': 'Linux-6.1.123+-x86_64-with-glibc2.35'}\n",
            "INFO:scrapy.addons:Enabled addons:\n",
            "[]\n",
            "2025-06-24 15:39:24 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "DEBUG:scrapy.utils.log:Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "DEBUG:scrapy.utils.log:Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "INFO:scrapy.extensions.telnet:Telnet Password: 4425c64ea741d4e8\n",
            "2025-06-24 15:39:24 [scrapy.extensions.telnet] INFO: Telnet Password: 4425c64ea741d4e8\n",
            "INFO:scrapy.middleware:Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.closespider.CloseSpider',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2025-06-24 15:39:24 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.closespider.CloseSpider',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "INFO:scrapy.crawler:Overridden settings:\n",
            "{'CLOSESPIDER_ITEMCOUNT': 20,\n",
            " 'DOWNLOAD_DELAY': 0.25,\n",
            " 'LOG_LEVEL': 'INFO',\n",
            " 'USER_AGENT': 'MyCustomBot/1.0 (+http://mywebsite.com/botinfo)'}\n",
            "2025-06-24 15:39:24 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'CLOSESPIDER_ITEMCOUNT': 20,\n",
            " 'DOWNLOAD_DELAY': 0.25,\n",
            " 'LOG_LEVEL': 'INFO',\n",
            " 'USER_AGENT': 'MyCustomBot/1.0 (+http://mywebsite.com/botinfo)'}\n",
            "INFO:scrapy.middleware:Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2025-06-24 15:39:24 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "INFO:scrapy.middleware:Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n",
            " 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2025-06-24 15:39:24 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n",
            " 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "INFO:scrapy.middleware:Enabled item pipelines:\n",
            "[]\n",
            "2025-06-24 15:39:24 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Scrapy process...\n"
          ]
        },
        {
          "ename": "ReactorNotRestartable",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-21-292871532.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting Scrapy process...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This should now work multiple times in the same session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Scrapy process finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scrapy/crawler.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[1;32m    500\u001b[0m                 \u001b[0;34m\"after\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"startup\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstall_shutdown_handlers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signal_shutdown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             )\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstall_signal_handlers\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_graceful_stop_reactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDeferred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/twisted/internet/asyncioreactor.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asyncioEventloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_forever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_justStopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorAlreadyRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_startedBefore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorNotRestartable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muninstall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mReactorNotRestartable\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}